{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e5a15b-8163-4d5a-b24d-00d2e35289c8",
   "metadata": {},
   "source": [
    "## firebase setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4e714d-7ec0-4094-8fdb-dde36c608f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n",
      "Firebase Admin SDK initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/google/api_core/_python_version_support.py:237: FutureWarning: You are using a non-supported Python version (3.8.10). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import os\n",
    "\n",
    "SERVICE_ACCOUNT_KEY_PATH = os.environ.get(\"FIREBASE_SERVICE_ACCOUNT_KEY_PATH\", \"./cstam2-1f2ec-firebase-adminsdk-fbsvc-2ab61a7ed6.json\")\n",
    "\n",
    "try:\n",
    "    # Check if the default app is already initialized\n",
    "    app = firebase_admin.get_app()\n",
    "    print(\"Firebase Admin SDK already initialized. Reusing existing app instance.\")\n",
    "except ValueError:\n",
    "    # If not initialized, proceed with initialization\n",
    "    try:\n",
    "        cred = credentials.Certificate(SERVICE_ACCOUNT_KEY_PATH)\n",
    "        app = firebase_admin.initialize_app(cred)\n",
    "        print(\"Firebase Admin SDK initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Firebase Admin SDK initialization: {e}\")\n",
    "        # It's crucial to handle this error, as your app can't write to Firestore without it.\n",
    "        raise # Re-raise to stop the script if Firebase initialization fails\n",
    "\n",
    "db = firestore.client(app=app) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31353cda-6ad2-4fc9-bd22-1128649b16c6",
   "metadata": {},
   "source": [
    "## setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44e757d-cf9a-4a21-8542-cb7acbca2b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/12 19:56:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/12 19:56:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, to_timestamp, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType\n",
    "import time\n",
    "import logging\n",
    "# ---------- CONFIG ----------\n",
    "KAFKA_BOOTSTRAP = \"kafka:29092\"                  \n",
    "\n",
    "CHECKPOINT_BASE = \"/data/checkpoints\"\n",
    "\n",
    "# Configure logger\n",
    "logger = logging.getLogger(\"DataCleaning\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)\n",
    "# ---------- Spark session ----------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"health-streams-to-firebase\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# ---------- utils ----------\n",
    "def read_topic(topic, schema):\n",
    "    return (\n",
    "        spark.readStream\n",
    "             .format(\"kafka\")\n",
    "             .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "             .option(\"subscribe\", topic)\n",
    "             .option(\"startingOffsets\", \"latest\")\n",
    "             .load()\n",
    "             .selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "             .select(from_json(col(\"json_str\"), schema).alias(\"data\"))   # <<< fixed\n",
    "             .select(\"data.*\")\n",
    "    )\n",
    "\n",
    "\n",
    "heart_rate_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"heart_rate\", IntegerType(), True)\n",
    "])\n",
    "steps_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"steps\", IntegerType(), True)\n",
    "])\n",
    "calories_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"calories\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77f9ab-c70f-427e-9b00-18eb72014d60",
   "metadata": {},
   "source": [
    "## cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cd1631-e5bf-465b-b1c0-c413aac3836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-12 19:56:08,227] INFO - Cleaning heart rate stream...\n",
      "[2025-11-12 19:56:08,261] INFO - Normalizing timestamp — candidates: ['time']\n",
      "[2025-11-12 19:56:08,261] INFO - Using 'time' as timestamp source.\n",
      "[2025-11-12 19:56:08,352] INFO - Heart rate cleaning complete (streaming mode).\n",
      "[2025-11-12 19:56:08,353] INFO - Cleaning calories stream...\n",
      "[2025-11-12 19:56:08,368] INFO - Normalizing timestamp — candidates: ['time']\n",
      "[2025-11-12 19:56:08,368] INFO - Using 'time' as timestamp source.\n",
      "[2025-11-12 19:56:08,494] INFO - Calories cleaning complete (streaming mode).\n",
      "[2025-11-12 19:56:08,496] INFO - Cleaning steps stream...\n",
      "[2025-11-12 19:56:08,523] INFO - Normalizing timestamp — candidates: ['time']\n",
      "[2025-11-12 19:56:08,524] INFO - Using 'time' as timestamp source.\n",
      "[2025-11-12 19:56:08,580] INFO - Steps cleaning complete (streaming mode).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "## ---------- timestamp normalization & cleaning ----------\n",
    "def normalize_timestamp(df):\n",
    "    candidates = [c for c in [\"timestamp\", \"ts\", \"ts_ms\", \"time\", \"created_at\"] if c in df.columns]\n",
    "    logger.info(f\"Normalizing timestamp — candidates: {candidates}\")\n",
    "\n",
    "    if not candidates:\n",
    "        logger.warning(\"No timestamp found — creating current-time column.\")\n",
    "        return df.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "    src = candidates[0]\n",
    "    logger.info(f\"Using '{src}' as timestamp source.\")\n",
    "    src_col = col(src)\n",
    "\n",
    "    # Only convert numeric timestamps; keep native timestamps unchanged\n",
    "    df = df.withColumn(\n",
    "        \"timestamp\",\n",
    "        when(src_col.cast(DoubleType()).isNotNull(), \n",
    "             to_timestamp(from_unixtime(\n",
    "                 (src_col.cast(DoubleType()) / when(src_col.cast(DoubleType()) > 1e12, lit(1000.0)).otherwise(lit(1.0))).cast(\"long\")\n",
    "             ))\n",
    "        ).otherwise(src_col.cast(TimestampType()))\n",
    "    )\n",
    "\n",
    "    return df.drop(src)\n",
    "def clean_heart_rate(df):\n",
    "    logger.info(\"Cleaning heart rate stream...\")\n",
    "    df = df.withColumn(\"heart_rate\", col(\"heart_rate\").cast(IntegerType()))\n",
    "    df = normalize_timestamp(df)\n",
    "    df = df.dropna(subset=[\"heart_rate\", \"timestamp\"])\n",
    "    df = df.filter((col(\"heart_rate\") > 0) & (col(\"heart_rate\") < 300))\n",
    "    logger.info(\"Heart rate cleaning complete (streaming mode).\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_calories(df):\n",
    "    logger.info(\"Cleaning calories stream...\")\n",
    "    df = df.withColumn(\"calories\", col(\"calories\").cast(DoubleType()))\n",
    "    df = normalize_timestamp(df)\n",
    "    df = df.dropna(subset=[\"calories\", \"timestamp\"])\n",
    "    df = df.filter(col(\"calories\") >= 0)\n",
    "    logger.info(\"Calories cleaning complete (streaming mode).\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_steps(df):\n",
    "    logger.info(\"Cleaning steps stream...\")\n",
    "    df = df.withColumn(\"steps\", col(\"steps\").cast(IntegerType()))\n",
    "    df = normalize_timestamp(df)\n",
    "    df = df.dropna(subset=[\"steps\", \"timestamp\"])\n",
    "    df = df.filter(col(\"steps\") >= 0)\n",
    "    logger.info(\"Steps cleaning complete (streaming mode).\")\n",
    "    return df\n",
    "    \n",
    "\n",
    "  \n",
    "# ---------- prepare streams ----------\n",
    "\n",
    "heart_rate_df = read_topic(\"heartrate\", heart_rate_schema)\n",
    "calories_df = read_topic(\"calories\", calories_schema)\n",
    "steps_df = read_topic(\"steps\", steps_schema)\n",
    "\n",
    "hr_clean = clean_heart_rate(heart_rate_df)\n",
    "cal_clean = clean_calories(calories_df)\n",
    "st_clean = clean_steps(steps_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27bb1e7-61f3-4f9a-a692-dfa5228de57b",
   "metadata": {},
   "source": [
    "## anomaly detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea262293-e537-42b8-aaed-c2f967138854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def send_alert_to_firestore(user_id, alert_type, value, firestore_client,timestamp):\n",
    "    \"\"\"\n",
    "    Write an anomaly alert to Firestore.\n",
    "    Each alert becomes a document in the 'alerts' collection.\n",
    "\n",
    "    Args:\n",
    "        user_id (str): The ID of the affected user.\n",
    "        alert_type (str): Type of the alert, e.g., 'heart_rate', 'calories', etc.\n",
    "        firestore_client: Initialized Firestore client (google.cloud.firestore.Client).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        alerts_ref = firestore_client.collection(\"alerts\")\n",
    "        alert_doc = {\n",
    "            \"user_id\": user_id,\n",
    "            \"type\": alert_type,\n",
    "            \"value\": value,\n",
    "            \"detected_at\": timestamp,\n",
    "            \"alert_created_at_unix\": int(time.time()),\n",
    "            \"status\":\"waiting\"\n",
    "        }\n",
    "\n",
    "        alerts_ref.add(alert_doc)\n",
    "        print(f\"[ok] Alert written for user={user_id}: {alert_type} — {value}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[error] Failed to write alert for user={user_id}: {e}\")\n",
    "        \n",
    "def detect_heart_rate_anomalies(df, firestore_client):\n",
    "    anomalies = df.filter((col(\"heart_rate\") > 180) | (col(\"heart_rate\") < 45))\n",
    "    if anomalies.rdd.isEmpty():\n",
    "        return\n",
    "    for row in anomalies.toLocalIterator():\n",
    "        send_alert_to_firestore(\n",
    "            user_id=row.user_id,\n",
    "            alert_type=\"heart_rate\",\n",
    "            value=row.heart_rate,\n",
    "            firestore_client=firestore_client,\n",
    "            timestamp= row.timestamp\n",
    "        )\n",
    "        \n",
    "def detect_low_calories(df, firestore_client):\n",
    "    low_activity = df.filter(col(\"calories\") < 0.5)  # e.g. less than 0.5 kcal/min or similar\n",
    "    if low_activity.rdd.isEmpty():\n",
    "        return\n",
    "    for row in low_activity.toLocalIterator():\n",
    "        send_alert_to_firestore(\n",
    "            user_id=row.user_id,\n",
    "            alert_type=\"low_calories\",\n",
    "            value=row.calories,\n",
    "            firestore_client=firestore_client,\n",
    "            timestamp= row.timestamp\n",
    "        )\n",
    "        \n",
    "def detect_inactivity(df, firestore_client):\n",
    "    inactive = df.filter(col(\"steps\") == 0)\n",
    "    if inactive.rdd.isEmpty():\n",
    "        return\n",
    "    for row in inactive.toLocalIterator():\n",
    "        send_alert_to_firestore(\n",
    "            user_id=row.user_id,\n",
    "            alert_type=\"inactivity\",\n",
    "            value=0,\n",
    "            firestore_client=firestore_client,\n",
    "            timestamp= row.timestamp\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cbe86-3995-45da-95ed-e7ca6cb5d2fd",
   "metadata": {},
   "source": [
    "## write stream functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ee6899-285e-4a0f-969f-8dcf7d116fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import decimal\n",
    "import json\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError\n",
    "\n",
    "# logger (reuse your existing logger or this)\n",
    "fs_logger = logging.getLogger(\"firestore-writer\")\n",
    "fs_logger.setLevel(logging.INFO)\n",
    "if not fs_logger.hasHandlers():\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\"))\n",
    "    fs_logger.addHandler(h)\n",
    "\n",
    "\n",
    "def _serialize_value(v):\n",
    "    \"\"\"Make values Firestore-safe: datetime -> ISO, Decimal -> float, fallback to str for non-JSON.\"\"\"\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (datetime.datetime, datetime.date)):\n",
    "        return v.isoformat()\n",
    "    if isinstance(v, decimal.Decimal):\n",
    "        return float(v)\n",
    "    # Bytes -> decode\n",
    "    if isinstance(v, (bytes, bytearray)):\n",
    "        try:\n",
    "            return v.decode()\n",
    "        except Exception:\n",
    "            return str(v)\n",
    "    # Test JSON-serializable quickly\n",
    "    try:\n",
    "        json.dumps(v)\n",
    "        return v\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "\n",
    "def _commit_with_retries(batch_obj, max_retries=3, base_backoff=0.5):\n",
    "    \"\"\"Commit a Firestore batch with retries on transient errors.\"\"\"\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch_obj.commit()\n",
    "            return\n",
    "        except (GoogleAPICallError, RetryError, IOError) as e:\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                fs_logger.exception(\"Firestore commit failed after %d attempts\", attempt - 1)\n",
    "                raise\n",
    "            backoff = base_backoff * (2 ** (attempt - 1))\n",
    "            fs_logger.warning(\"Transient error committing firestore batch (attempt %d). Backing off %.2fs. Error: %s\",\n",
    "                              attempt, backoff, str(e))\n",
    "            time.sleep(backoff)\n",
    "\n",
    "\n",
    "def make_firestore_writer(collection_name, firestore_client, batch_size=500, max_retries=3):\n",
    "    \"\"\"\n",
    "    Writes to Firestore under this structure:\n",
    "      users/{user_id}/{collection_name}/{auto_doc_id}\n",
    "\n",
    "    collection_name: e.g. \"heart_rate\", \"calories\", \"steps\"\n",
    "    \"\"\"\n",
    "    def write_batch_to_firestore(batch_df, epoch_id):\n",
    "        epoch_str = str(epoch_id) if epoch_id is not None else \"(no-epoch)\"\n",
    "        try:\n",
    "            rows = batch_df.count()\n",
    "        except Exception as e:\n",
    "            fs_logger.exception(\"[epoch %s] failed to count batch: %s\", epoch_str, e)\n",
    "            rows = None\n",
    "\n",
    "        if not rows:\n",
    "            fs_logger.info(\"[epoch %s] empty, skipping collection=%s\", epoch_str, collection_name)\n",
    "            return\n",
    "\n",
    "        fs_logger.info(\"[epoch %s] writing %s rows to Firestore subcollections '%s'\", epoch_str, rows, collection_name)\n",
    "\n",
    "        docs_written = 0\n",
    "        ops_in_current_batch = 0\n",
    "        fs_batch = firestore_client.batch()\n",
    "\n",
    "        try:\n",
    "            for row in batch_df.toLocalIterator():\n",
    "                data = row.asDict(recursive=True)\n",
    "                user_id = data.get(\"user_id\")\n",
    "\n",
    "                if not user_id:\n",
    "                    fs_logger.warning(\"[epoch %s] skipping row without user_id: %s\", epoch_str, data)\n",
    "                    continue\n",
    "\n",
    "                # Serialize all values to Firestore-safe types\n",
    "                for k, v in list(data.items()):\n",
    "                    data[k] = _serialize_value(v)\n",
    "\n",
    "                # Path: users/{user_id}/{collection_name}/{auto_doc_id}\n",
    "                doc_ref = (\n",
    "                    firestore_client.collection(\"users\")\n",
    "                    .document(str(user_id))\n",
    "                    .collection(collection_name)\n",
    "                    .document()\n",
    "                )\n",
    "\n",
    "                fs_batch.set(doc_ref, data)\n",
    "                ops_in_current_batch += 1\n",
    "\n",
    "                if ops_in_current_batch >= batch_size:\n",
    "                    _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "                    docs_written += ops_in_current_batch\n",
    "                    ops_in_current_batch = 0\n",
    "                    fs_batch = firestore_client.batch()\n",
    "\n",
    "            if ops_in_current_batch > 0:\n",
    "                _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "                docs_written += ops_in_current_batch\n",
    "\n",
    "            fs_logger.info(\"[epoch %s] wrote %d docs under users/*/%s/\", epoch_str, docs_written, collection_name)\n",
    "\n",
    "        except Exception as e:\n",
    "            fs_logger.exception(\"[epoch %s] Firestore write failed: %s\", epoch_str, e)\n",
    "            raise\n",
    "\n",
    "    return write_batch_to_firestore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbfadc-5a23-49a9-941c-1e93d94254d1",
   "metadata": {},
   "source": [
    "## cleaning stream queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf1e47a-ec65-4bbc-970c-90dbc340312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-12 19:56:19,225 - [epoch 15305] writing 20 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:19,228 - [epoch 269] writing 1 rows to Firestore subcollections 'steps'\n",
      "[INFO] 2025-11-12 19:56:19,230 - [epoch 269] writing 1 rows to Firestore subcollections 'calories'\n",
      "[INFO] 2025-11-12 19:56:20,330 - [epoch 269] wrote 1 docs under users/*/steps/\n",
      "[INFO] 2025-11-12 19:56:20,346 - [epoch 269] wrote 1 docs under users/*/calories/\n",
      "[INFO] 2025-11-12 19:56:20,349 - [epoch 15305] wrote 20 docs under users/*/heart_rate/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Alert written for user=1503960366: inactivity — 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-12 19:56:20,854 - [epoch 15306] writing 5 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:21,564 - [epoch 15306] wrote 5 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:21,861 - [epoch 15307] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:22,036 - [epoch 15307] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:22,707 - [epoch 15308] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:22,944 - [epoch 15308] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:23,597 - [epoch 15309] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:23,958 - [epoch 15309] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:24,608 - [epoch 15310] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:24,968 - [epoch 15310] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:25,614 - [epoch 15311] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:25,971 - [epoch 15311] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:26,587 - [epoch 15312] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:26,971 - [epoch 15312] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:27,585 - [epoch 15313] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:27,974 - [epoch 15313] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:28,582 - [epoch 15314] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:28,982 - [epoch 15314] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:29,592 - [epoch 15315] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:29,982 - [epoch 15315] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:30,596 - [epoch 15316] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:31,004 - [epoch 15316] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:31,601 - [epoch 15317] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:31,977 - [epoch 15317] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:32,592 - [epoch 15318] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:33,001 - [epoch 15318] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:33,593 - [epoch 15319] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:34,014 - [epoch 15319] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:34,620 - [epoch 15320] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:35,075 - [epoch 15320] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:35,696 - [epoch 15321] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:36,023 - [epoch 15321] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:36,627 - [epoch 15322] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:37,031 - [epoch 15322] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:37,634 - [epoch 15323] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[INFO] 2025-11-12 19:56:38,100 - [epoch 15323] wrote 1 docs under users/*/heart_rate/\n",
      "INFO:firestore-writer:[epoch 15323] wrote 1 docs under users/*/heart_rate/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     37\u001b[0m cal_alerts \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     38\u001b[0m     cal_clean\u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(\u001b[38;5;28;01mlambda\u001b[39;00m batch_df, epoch_id: detect_low_calories(batch_df, db))\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHECKPOINT_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/alerts_calories\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m st_alerts \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     45\u001b[0m     st_clean\u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(\u001b[38;5;28;01mlambda\u001b[39;00m batch_df, epoch_id: detect_inactivity(batch_df, db))\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHECKPOINT_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/alerts_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     49\u001b[0m )\n\u001b[0;32m---> 51\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarted all streams. Waiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m spark\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mawaitAnyTermination()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/streaming/query.py:596\u001b[0m, in \u001b[0;36mStreamingQueryManager.awaitAnyTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsqm\u001b[38;5;241m.\u001b[39mawaitAnyTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsqm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-12 19:56:38,791 - [epoch 15324] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "INFO:firestore-writer:[epoch 15324] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:39,029 - [epoch 15324] wrote 1 docs under users/*/heart_rate/\n",
      "INFO:firestore-writer:[epoch 15324] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:39,632 - [epoch 15325] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "INFO:firestore-writer:[epoch 15325] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:40,040 - [epoch 15325] wrote 1 docs under users/*/heart_rate/\n",
      "INFO:firestore-writer:[epoch 15325] wrote 1 docs under users/*/heart_rate/\n",
      "[INFO] 2025-11-12 19:56:40,636 - [epoch 15326] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "INFO:firestore-writer:[epoch 15326] writing 1 rows to Firestore subcollections 'heart_rate'\n",
      "[INFO] 2025-11-12 19:56:41,051 - [epoch 15326] wrote 1 docs under users/*/heart_rate/\n",
      "INFO:firestore-writer:[epoch 15326] wrote 1 docs under users/*/heart_rate/\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_48/738261257.py\", line 32, in <lambda>\n",
      "    .foreachBatch(lambda batch_df, epoch_id: detect_heart_rate_anomalies(batch_df, db))\n",
      "  File \"/tmp/ipykernel_48/1089363820.py\", line 33, in detect_heart_rate_anomalies\n",
      "    if anomalies.rdd.isEmpty():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 2920, in isEmpty\n",
      "    return self.getNumPartitions() == 0 or len(self.take(1)) == 0\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(Unknown Source)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:975)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor94.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy30.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start streams separately with their own checkpoints\n",
    "# Pass the initialized Firestore client 'db' to your writer functions\n",
    "hr_query = (\n",
    "    hr_clean\n",
    "    .writeStream\n",
    "    .foreachBatch(make_firestore_writer(\"heart_rate\", db)) # Pass the Firestore client here\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/heart_rate\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "cal_query = (\n",
    "    cal_clean\n",
    "    .writeStream\n",
    "    .foreachBatch(make_firestore_writer(\"calories\", db)) # Pass the Firestore client here\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/calories\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "st_query = (\n",
    "    st_clean\n",
    "    .writeStream\n",
    "    .foreachBatch(make_firestore_writer(\"steps\", db)) # Pass the Firestore client here\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/steps\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "# Start streams for each detection type\n",
    "hr_alerts = (\n",
    "    hr_clean.writeStream\n",
    "    .foreachBatch(lambda batch_df, epoch_id: detect_heart_rate_anomalies(batch_df, db))\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/alerts_hr\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "cal_alerts = (\n",
    "    cal_clean.writeStream\n",
    "    .foreachBatch(lambda batch_df, epoch_id: detect_low_calories(batch_df, db))\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/alerts_calories\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "st_alerts = (\n",
    "    st_clean.writeStream\n",
    "    .foreachBatch(lambda batch_df, epoch_id: detect_inactivity(batch_df, db))\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/alerts_steps\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "spark.streams.awaitAnyTermination()\n",
    "\n",
    "\n",
    "print(\"Started all streams. Waiting...\")\n",
    "spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66dafa17-a647-4026-9ec1-0488cee116b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stopped all active queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "print(\"✅ Stopped all active queries.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
