{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efacd44c-165d-412e-a604-fd9a1c1447db",
   "metadata": {},
   "source": [
    "# data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf0c548-4dd7-49d5-ba1a-0bd3d7f3d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+------------+---------+---------------+---------+--------+-----------+\n",
      "|user_id   |date      |meal_type|food_name   |water_ml |sport_available|weight_kg|height_m|sleep_hours|\n",
      "+----------+----------+---------+------------+---------+---------------+---------+--------+-----------+\n",
      "|5577150313|2025-11-01|Breakfast|Oats        |434.5987 |false          |65.9     |1.83    |8.0        |\n",
      "|5577150313|2025-11-01|Lunch    |Nuts        |548.94495|false          |65.9     |1.83    |8.0        |\n",
      "|5577150313|2025-11-01|Dinner   |Spinach     |652.39   |false          |65.9     |1.83    |8.0        |\n",
      "|5577150313|2025-11-01|Snack    |Salmon      |254.29272|false          |65.9     |1.83    |8.0        |\n",
      "|5577150313|2025-11-02|Breakfast|Apple       |334.9065 |false          |66.7     |1.83    |7.3        |\n",
      "|5577150313|2025-11-02|Lunch    |Nuts        |533.6832 |false          |66.7     |1.83    |7.3        |\n",
      "|5577150313|2025-11-02|Dinner   |Salmon      |441.9292 |false          |66.7     |1.83    |7.3        |\n",
      "|5577150313|2025-11-02|Snack    |Spinach     |153.07475|false          |66.7     |1.83    |7.3        |\n",
      "|5577150313|2025-11-03|Breakfast|Orange Juice|477.72607|false          |66.8     |1.83    |4.6        |\n",
      "|5577150313|2025-11-03|Lunch    |Orange      |587.6817 |false          |66.8     |1.83    |4.6        |\n",
      "+----------+----------+---------+------------+---------+---------------+---------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+----------+---------------------+-----------------+-----------------------+-------------+---------------+----------------+-----------------+----------------------+-----------------------+-----------+------------+---------------------+\n",
      "|   user_id|      date|total_Calories (kcal)|total_Protein (g)|total_Carbohydrates (g)|total_Fat (g)|total_Fiber (g)|total_Sugars (g)|total_Sodium (mg)|total_Cholesterol (mg)|total_Water_Intake (ml)|sleep_hours|target_steps|target_calories_burnt|\n",
      "+----------+----------+---------------------+-----------------+-----------------------+-------------+---------------+----------------+-----------------+----------------------+-----------------------+-----------+------------+---------------------+\n",
      "|5577150313|2025-11-01|               2540.0|        111.57775|              345.90155|     82.03519|      25.989065|        62.86658|        2194.7798|             285.11057|              2873.9324|  7.3781657|      9923.0|               2483.0|\n",
      "|5577150313|2025-11-02|               2520.0|        109.38798|              362.73743|     83.99973|       26.31368|       62.973293|          2198.35|             301.05807|              2828.9812|   7.253315|      9989.0|               2450.0|\n",
      "|5577150313|2025-11-03|               2518.0|       108.974144|              350.06894|     76.25855|       27.92352|        64.44428|        2131.9053|             305.91788|              2756.0771|   7.211326|     10839.0|               2427.0|\n",
      "|5577150313|2025-11-04|               2505.0|        105.50499|               382.9934|     78.80193|      26.961243|        64.60527|        2242.7664|              310.0193|              2771.7266|   7.481113|      9886.0|               2392.0|\n",
      "|5577150313|2025-11-05|               2445.0|        106.39052|              347.85965|     76.95301|      25.868464|        56.28376|        2096.1865|              286.8202|              2759.9404|   7.150263|      9933.0|               2364.0|\n",
      "|5577150313|2025-11-06|               2529.0|        109.35839|              370.41095|     78.61901|      27.955185|       56.834003|        2260.8337|              303.4932|              2793.6023|   7.846838|     10130.0|               2326.0|\n",
      "|5577150313|2025-11-07|               2396.0|        104.14917|               382.7672|     80.07037|      27.339285|        60.03251|        2100.8462|             307.58832|              2871.2688|   7.595772|      9727.0|               2385.0|\n",
      "|5577150313|2025-11-08|               2421.0|        106.94571|              352.44534|     81.72177|      27.884716|         56.1641|        2286.8765|              296.3928|               2901.224|  7.2285643|     10949.0|               2416.0|\n",
      "|5577150313|2025-11-09|               2454.0|        104.78831|              355.05322|     84.91654|      28.066801|       63.740643|         2127.508|              280.7562|              2901.6746|   7.948402|     10777.0|               2495.0|\n",
      "|5577150313|2025-11-10|               2511.0|        106.90932|              345.51352|    84.940926|      26.425117|         64.8246|        2178.4675|             282.50598|              2865.7788|  7.9454317|      9516.0|               2418.0|\n",
      "+----------+----------+---------------------+-----------------+-----------------------+-------------+---------------+----------------+-----------------+----------------------+-----------------------+-----------+------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, DateType\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "# --- Load your real meal dataset ---\n",
    "food_df = spark.read.csv(\"./food_nutrition_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Expected columns: e.g., [\"food_name\", \"meal_type\", \"calories\", \"protein\", ...]\n",
    "# If your column names differ, adjust below\n",
    "meal_types = [\"Breakfast\", \"Lunch\", \"Dinner\", \"Snack\"]\n",
    "\n",
    "# --- Define realistic water intake ranges (ml) per meal type ---\n",
    "water_ranges = {\n",
    "    \"Breakfast\": (250, 500),\n",
    "    \"Lunch\": (400, 700),\n",
    "    \"Dinner\": (400, 700),\n",
    "    \"Snack\": (150, 300)\n",
    "}\n",
    "\n",
    "# --- Parameters ---\n",
    "start_date = datetime.date(2025, 11, 1)\n",
    "end_date = datetime.date(2025, 11, 30)\n",
    "num_users = 5  # adjustable\n",
    "\n",
    "# --- Convert food_df to Python lists for random sampling ---\n",
    "food_by_type = {}\n",
    "for m in meal_types:\n",
    "    food_by_type[m] = (\n",
    "        food_df.filter(food_df[\"meal_type\"] == m)\n",
    "        .select(\"food_item\")\n",
    "        .rdd.flatMap(lambda x: x)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "# --- Generate dummy data ---\n",
    "rows = []\n",
    "users_ids = [5577150313,5553957443,4020332650]\n",
    "for user_idx in users_ids:\n",
    "    user_id = f\"{user_idx}\"\n",
    "    height = round(random.uniform(1.65, 1.85), 2)\n",
    "    base_weight = round(random.uniform(60, 85), 1)\n",
    "\n",
    "    for day_offset in range((end_date - start_date).days + 1):\n",
    "        day = start_date + datetime.timedelta(days=day_offset)\n",
    "        sport_available = bool(random.choice([True, False]))\n",
    "        weight_today = round(base_weight + random.uniform(-1, 1), 1)\n",
    "        sleep_hours = round(random.uniform(4.5, 9.0), 1)\n",
    "\n",
    "        for meal_type in meal_types:\n",
    "            foods = food_by_type.get(meal_type, [])\n",
    "            food_choice = random.choice(foods) if foods else f\"Random {meal_type}\"\n",
    "            water_ml = random.uniform(*water_ranges[meal_type])\n",
    "            rows.append((user_id, day, meal_type, food_choice, water_ml,\n",
    "                         sport_available, weight_today, height, sleep_hours))\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"meal_type\", StringType(), True),\n",
    "    StructField(\"food_name\", StringType(), True),\n",
    "    StructField(\"water_ml\", FloatType(), True),\n",
    "    StructField(\"sport_available\", BooleanType(), True),\n",
    "    StructField(\"weight_kg\", FloatType(), True),\n",
    "    StructField(\"height_m\", FloatType(), True),\n",
    "    StructField(\"sleep_hours\", FloatType(), True)\n",
    "])\n",
    "plan_rows = []\n",
    "for user_idx in users_ids:\n",
    "    user_id = f\"{user_idx}\"\n",
    "\n",
    "    # base daily targets (adjustable per user)\n",
    "    base_targets = {\n",
    "        \"Calories\": float(random.randint(2000, 2700)),\n",
    "        \"Protein\": float(random.randint(90, 130)),\n",
    "        \"Carbohydrates\": float(random.randint(250, 400)),\n",
    "        \"Fat\": float(random.randint(60, 90)),\n",
    "        \"Fiber\": float(random.randint(25, 35)),\n",
    "        \"Sugars\": float(random.randint(40, 70)),\n",
    "        \"Sodium\": float(random.randint(1500, 2500)),\n",
    "        \"Cholesterol\": float(random.randint(200, 300)),\n",
    "        \"Water_Intake\": float(random.randint(2000, 3000)),\n",
    "        \"Sleep\": float(random.uniform(6.5, 8.0)),\n",
    "        \"Steps\": float(random.randint(7000, 12000)),\n",
    "        \"Calories_Burnt\": float(random.randint(1800, 2500))\n",
    "    }\n",
    "\n",
    "    for day_offset in range((end_date - start_date).days + 1):\n",
    "        day = start_date + datetime.timedelta(days=day_offset)\n",
    "\n",
    "        # small day-to-day variation\n",
    "        row = (\n",
    "            user_id,\n",
    "            day,\n",
    "           float(base_targets[\"Calories\"] + random.randint(-100, 100)),\n",
    "            float(base_targets[\"Protein\"] + random.uniform(-5, 5)),\n",
    "            float(base_targets[\"Carbohydrates\"] + random.uniform(-20, 20)),\n",
    "            float(base_targets[\"Fat\"] + random.uniform(-5, 5)),\n",
    "            float(base_targets[\"Fiber\"] + random.uniform(-2, 2)),\n",
    "            float(base_targets[\"Sugars\"] + random.uniform(-5, 5)),\n",
    "            float(base_targets[\"Sodium\"] + random.uniform(-100, 100)),\n",
    "            float(base_targets[\"Cholesterol\"] + random.uniform(-20, 20)),\n",
    "            float(base_targets[\"Water_Intake\"] + random.uniform(-150, 150)),\n",
    "            float(base_targets[\"Sleep\"] + random.uniform(-0.5, 0.5)),\n",
    "            float(base_targets[\"Steps\"] + random.randint(-1000, 1000)),\n",
    "            float(base_targets[\"Calories_Burnt\"] + random.randint(-150, 150))\n",
    "        )\n",
    "\n",
    "        plan_rows.append(row)\n",
    "\n",
    "schema_plan = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"total_Calories (kcal)\", FloatType(), True),\n",
    "    StructField(\"total_Protein (g)\", FloatType(), True),\n",
    "    StructField(\"total_Carbohydrates (g)\", FloatType(), True),\n",
    "    StructField(\"total_Fat (g)\", FloatType(), True),\n",
    "    StructField(\"total_Fiber (g)\", FloatType(), True),\n",
    "    StructField(\"total_Sugars (g)\", FloatType(), True),\n",
    "    StructField(\"total_Sodium (mg)\", FloatType(), True),\n",
    "    StructField(\"total_Cholesterol (mg)\", FloatType(), True),\n",
    "    StructField(\"total_Water_Intake (ml)\", FloatType(), True),\n",
    "    StructField(\"sleep_hours\", FloatType(), True),\n",
    "    StructField(\"target_steps\", FloatType(), True),\n",
    "    StructField(\"target_calories_burnt\", FloatType(), True),\n",
    "])\n",
    "\n",
    "df_monthly_plan = spark.createDataFrame(plan_rows, schema_plan)\n",
    "# --- Create DataFrame ---\n",
    "df_daily_meals = spark.createDataFrame(rows, schema)\n",
    "\n",
    "# --- Show sample ---\n",
    "df_daily_meals.show(10, truncate=False)\n",
    "df_monthly_plan.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5beb46eb-9bfa-4284-bb18-525a622eefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import decimal\n",
    "import json\n",
    "from google.cloud import firestore\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, DateType\n",
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Logging Setup\n",
    "# ----------------------------\n",
    "fs_logger = logging.getLogger(\"firestore-writer\")\n",
    "fs_logger.setLevel(logging.INFO)\n",
    "if not fs_logger.hasHandlers():\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\"))\n",
    "    fs_logger.addHandler(h)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Helper Functions\n",
    "# ----------------------------\n",
    "def _serialize_value(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (datetime.datetime, datetime.date)):\n",
    "        return v.isoformat()\n",
    "    if isinstance(v, decimal.Decimal):\n",
    "        return float(v)\n",
    "    if isinstance(v, (bytes, bytearray)):\n",
    "        try:\n",
    "            return v.decode()\n",
    "        except Exception:\n",
    "            return str(v)\n",
    "    try:\n",
    "        json.dumps(v)\n",
    "        return v\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "\n",
    "def _commit_with_retries(batch_obj, max_retries=3, base_backoff=0.5):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch_obj.commit()\n",
    "            return\n",
    "        except (GoogleAPICallError, RetryError, IOError) as e:\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                fs_logger.exception(\"Firestore commit failed after %d attempts\", attempt - 1)\n",
    "                raise\n",
    "            backoff = base_backoff * (2 ** (attempt - 1))\n",
    "            fs_logger.warning(\n",
    "                \"Transient error committing firestore batch (attempt %d). Backing off %.2fs. Error: %s\",\n",
    "                attempt, backoff, str(e)\n",
    "            )\n",
    "            time.sleep(backoff)\n",
    "\n",
    "\n",
    "def make_firestore_writer(collection_name, firestore_client, batch_size=500, max_retries=3):\n",
    "    def write_batch_to_firestore(batch_df, epoch_id=None):\n",
    "        rows = batch_df.count()\n",
    "        if not rows:\n",
    "            fs_logger.info(\"[epoch %s] empty, skipping collection=%s\", str(epoch_id), collection_name)\n",
    "            return\n",
    "\n",
    "        fs_logger.info(\"[epoch %s] writing %s rows to Firestore collection '%s'\",\n",
    "                       str(epoch_id), rows, collection_name)\n",
    "\n",
    "        docs_written = 0\n",
    "        ops_in_current_batch = 0\n",
    "        fs_batch = firestore_client.batch()\n",
    "\n",
    "        for row in batch_df.toLocalIterator():\n",
    "            data = row.asDict(recursive=True)\n",
    "            user_id = data.get(\"user_id\")\n",
    "\n",
    "            if not user_id:\n",
    "                fs_logger.warning(\"[epoch %s] skipping row without user_id\", str(epoch_id))\n",
    "                continue\n",
    "\n",
    "            for k, v in list(data.items()):\n",
    "                data[k] = _serialize_value(v)\n",
    "\n",
    "            doc_ref = (\n",
    "                firestore_client.collection(\"users\")\n",
    "                .document(str(user_id))\n",
    "                .collection(collection_name)\n",
    "                .document()\n",
    "            )\n",
    "\n",
    "            fs_batch.set(doc_ref, data)\n",
    "            ops_in_current_batch += 1\n",
    "\n",
    "            if ops_in_current_batch >= batch_size:\n",
    "                _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "                docs_written += ops_in_current_batch\n",
    "                ops_in_current_batch = 0\n",
    "                fs_batch = firestore_client.batch()\n",
    "\n",
    "        if ops_in_current_batch > 0:\n",
    "            _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "            docs_written += ops_in_current_batch\n",
    "\n",
    "        fs_logger.info(\"[epoch %s] wrote %d docs under users/*/%s/\",\n",
    "                       str(epoch_id), docs_written, collection_name)\n",
    "    return write_batch_to_firestore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f3383e-d888-4f29-96ee-43b52a2be692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-10 13:43:06,499 - [epoch 2025-11-07] writing 360 rows to Firestore collection 'daily_meals'\n",
      "[WARNING] 2025-11-10 13:43:50,464 - Transient error committing firestore batch (attempt 1). Backing off 0.50s. Error: Timeout of 60.0s exceeded, last exception: 503 DNS resolution failed for firestore.googleapis.com: C-ares status is not ARES_SUCCESS qtype=AAAA name=firestore.googleapis.com is_balancer=0: Could not contact DNS servers\n",
      "[INFO] 2025-11-10 13:43:57,187 - [epoch 2025-11-07] wrote 360 docs under users/*/daily_meals/\n",
      "[INFO] 2025-11-10 13:43:57,511 - [epoch 2025-11-07] writing 90 rows to Firestore collection 'monthly_plan'\n",
      "[INFO] 2025-11-10 13:43:59,106 - [epoch 2025-11-07] wrote 90 docs under users/*/monthly_plan/\n",
      "[INFO] 2025-11-10 13:43:59,106 - ✅ Finished writing dummy data to Firestore.\n"
     ]
    }
   ],
   "source": [
    "writer = make_firestore_writer(\"daily_meals\", db)\n",
    "\n",
    "# Run the batch write\n",
    "writer(df_daily_meals, epoch_id=\"2025-11-07\")\n",
    "writer = make_firestore_writer(\"monthly_plan\", db)\n",
    "writer(df_monthly_plan, epoch_id=\"2025-11-07\")\n",
    "\n",
    "fs_logger.info(\"✅ Finished writing dummy data to Firestore.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
