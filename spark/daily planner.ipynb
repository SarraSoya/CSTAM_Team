{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416a4bc-8784-425d-ae45-80e9fa02ad15",
   "metadata": {},
   "source": [
    "## firebase setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78517a12-d753-4a00-b8d2-c465f080e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n",
      "Firebase Admin SDK initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/google/api_core/_python_version_support.py:237: FutureWarning: You are using a non-supported Python version (3.8.10). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import os\n",
    "\n",
    "SERVICE_ACCOUNT_KEY_PATH = os.environ.get(\"FIREBASE_SERVICE_ACCOUNT_KEY_PATH\", \"./cstam2-1f2ec-firebase-adminsdk-fbsvc-2ab61a7ed6.json\")\n",
    "\n",
    "try:\n",
    "    # Check if the default app is already initialized\n",
    "    app = firebase_admin.get_app()\n",
    "    print(\"Firebase Admin SDK already initialized. Reusing existing app instance.\")\n",
    "except ValueError:\n",
    "    # If not initialized, proceed with initialization\n",
    "    try:\n",
    "        cred = credentials.Certificate(SERVICE_ACCOUNT_KEY_PATH)\n",
    "        app = firebase_admin.initialize_app(cred)\n",
    "        print(\"Firebase Admin SDK initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Firebase Admin SDK initialization: {e}\")\n",
    "        # It's crucial to handle this error, as your app can't write to Firestore without it.\n",
    "        raise # Re-raise to stop the script if Firebase initialization fails\n",
    "\n",
    "db = firestore.client(app=app) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b4408-f73f-4f3c-a981-fc249d41a402",
   "metadata": {},
   "source": [
    "## setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90671972-b60d-4b63-bc0a-391fab472f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/14 20:18:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, to_timestamp, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logger = logging.getLogger(\"DataCleaning\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)\n",
    "# ---------- Spark session ----------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"health-streams-to-firebase\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "food_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49002d26-6b63-41ce-83b6-4f64c7f1f545",
   "metadata": {},
   "source": [
    "## import and calculate daily metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd33f8-9fbf-4089-8c52-5a32f4121170",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1fd3bb-b977-4c34-90e4-c2b941d00b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    avg,\n",
    "    expr,\n",
    "    to_timestamp,when, sum as _sum,\n",
    ")\n",
    "\n",
    "\n",
    "def get_user_ids(db, collection_name=\"users\", debug=False):\n",
    "    \"\"\"\n",
    "    Robustly enumerate top-level user document IDs.\n",
    "    Tries, in order:\n",
    "      1) collection_ref.list_documents()\n",
    "      2) collection_ref.get()\n",
    "      3) collection_group (search for nested 'users' if top-level empty)\n",
    "    Returns list of user_ids (strings).\n",
    "    \"\"\"\n",
    "    user_ids = []\n",
    "\n",
    "\n",
    "    try:\n",
    "        coll_ref = db.collection(collection_name)\n",
    "\n",
    "        # 1) Try list_documents() (lightweight, returns DocumentReference objects)\n",
    "        try:\n",
    "            docs = list(coll_ref.list_documents())\n",
    "            if docs:\n",
    "                user_ids = [d.id for d in docs]\n",
    "                if debug:\n",
    "                    print(f\"‚úÖ get_user_ids: using list_documents(), found {len(user_ids)} users.\")\n",
    "                return user_ids\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"‚ÑπÔ∏è list_documents() returned 0 refs; falling back to get().\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(\"‚ö†Ô∏è list_documents() failed:\", repr(e))\n",
    "\n",
    "        # 2) Fallback: collection.get() (reads documents)\n",
    "        try:\n",
    "            docs = list(coll_ref.get())\n",
    "            if docs:\n",
    "                user_ids = [d.id for d in docs]\n",
    "                if debug:\n",
    "                    print(f\"‚úÖ get_user_ids: using get(), found {len(user_ids)} users.\")\n",
    "                return user_ids\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"‚ÑπÔ∏è collection.get() returned 0 documents; falling back to collection_group().\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(\"‚ö†Ô∏è collection.get() failed:\", repr(e))\n",
    "\n",
    "        # 3) Last resort: collection_group search (if 'users' lives nested under other paths)\n",
    "        try:\n",
    "            group_docs = list(db.collection_group(collection_name).limit(500).stream())\n",
    "            if group_docs:\n",
    "                # Use parent document id if the structure is like parent/{parent_id}/users/{user_id}\n",
    "                user_ids = [d.reference.id for d in group_docs]\n",
    "                if debug:\n",
    "                    print(f\"‚úÖ get_user_ids: using collection_group('{collection_name}'), found {len(user_ids)} documents.\")\n",
    "                return user_ids\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"‚ÑπÔ∏è collection_group('{collection_name}') returned 0 results.\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(\"‚ö†Ô∏è collection_group() failed:\", repr(e))\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(\"‚ùå get_user_ids: unexpected error:\", repr(e))\n",
    "\n",
    "    # final: empty result\n",
    "    if debug:\n",
    "        print(\"‚ùå get_user_ids: no users found by any method.\")\n",
    "    return []\n",
    "\n",
    "# -------------------------\n",
    "# Example: read_all_users using get_user_ids\n",
    "# -------------------------\n",
    "def read_all_users(db, collection_name=\"users\", batch_size=500, debug=False):\n",
    "    \"\"\"\n",
    "    Read user subcollections per user id using robust enumeration.\n",
    "    Returns list of records (dicts) same as earlier fetch_user_data would produce.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_ids = get_user_ids(db, collection_name=collection_name, debug=debug)\n",
    "        if debug:\n",
    "            print(f\"üìã Found {len(user_ids)} user doc ids.\")\n",
    "\n",
    "        all_records = []\n",
    "        for user_id in user_ids:\n",
    "            if debug:\n",
    "                print(f\"Processing user: {user_id}\")\n",
    "            try:\n",
    "                # Reuse your existing fetch_user_data(db, user_id, ...) here.\n",
    "                # Example minimal call (replace with your function):\n",
    "                user_records = fetch_user_data(db, user_id, page_size=batch_size)  # assume function exists\n",
    "                if not user_records and debug:\n",
    "                    print(f\"‚ÑπÔ∏è No subcollection docs for user {user_id}.\")\n",
    "                all_records.extend(user_records)\n",
    "                time.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"‚ö†Ô∏è Failed reading data for user {user_id}: {e}\")\n",
    "        return all_records\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(\"‚ùå read_all_users error:\", repr(e))\n",
    "        return []# --- Pagination helper ---\n",
    "def paginate_collection(collection_ref, page_size=500):\n",
    "    try:\n",
    "        docs = collection_ref.limit(page_size).stream()\n",
    "        last_doc = None\n",
    "        while True:\n",
    "            batch = list(docs)\n",
    "            if not batch:\n",
    "                break\n",
    "            yield batch\n",
    "            last_doc = batch[-1]\n",
    "            docs = collection_ref.start_after(last_doc).limit(page_size).stream()\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Pagination failed:\", e)\n",
    "\n",
    "# --- Fetch a single user's subcollections with debugging ---\n",
    "def fetch_user_data(db, user_id, page_size=500, debug=False):\n",
    "    user_ref = db.collection(\"users\").document(user_id)\n",
    "    records = []\n",
    "    for metric in [\"calories\", \"heart_rate\", \"steps\"]:\n",
    "        try:\n",
    "            sub_ref = user_ref.collection(metric)\n",
    "            all_docs = []\n",
    "            for batch in paginate_collection(sub_ref, page_size):\n",
    "                for doc in batch:\n",
    "                    all_docs.append(doc)\n",
    "            if debug:\n",
    "                print(f\"üß© User {user_id} | {metric}: {len(all_docs)} docs\")\n",
    "\n",
    "            for doc in all_docs:\n",
    "                data = doc.to_dict() or {}\n",
    "                if debug and data:\n",
    "                    print(f\"   ‚Ü≥ sample doc: {data}\")\n",
    "\n",
    "                # detect possible timestamp and value fields\n",
    "                timestamp_field = None\n",
    "                for k in [\"timestamp\", \"time\", \"created_at\", \"date\"]:\n",
    "                    if k in data:\n",
    "                        timestamp_field = k\n",
    "                        break\n",
    "\n",
    "                value_field = None\n",
    "                for k in [\"value\", \"heart_rate\", \"calories\", \"steps\", \"count\"]:\n",
    "                    if k in data:\n",
    "                        value_field = k\n",
    "                        break\n",
    "\n",
    "                if not timestamp_field or not value_field:\n",
    "                    if debug:\n",
    "                        print(f\"   ‚ö†Ô∏è Missing timestamp/value in {metric} doc: {data}\")\n",
    "                    continue\n",
    "\n",
    "                rec = {\n",
    "                    \"user_id\": user_id,\n",
    "                    \"metric_type\": metric,\n",
    "                    \"timestamp\": data[timestamp_field],\n",
    "                    \"value\": float(data[value_field]) if data[value_field] is not None else 0.0,\n",
    "                }\n",
    "                records.append(rec)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {metric} for user {user_id}: {e}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "\n",
    "# --- Build DataFrame safely ---\n",
    "def build_dataframe(spark, raw_data):\n",
    "    schema = StructType([\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"metric_type\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True),\n",
    "    ])\n",
    "    try:\n",
    "        df = spark.createDataFrame(raw_data or [], schema=schema)\n",
    "        print(f\"‚úÖ DataFrame created with {df.count()} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error creating DataFrame:\", e)\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    # --- Safely cast string timestamp -> proper timestamp ---\n",
    "    try:\n",
    "        df = df.withColumn(\n",
    "            \"timestamp\",\n",
    "            to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Failed to cast timestamp column:\", repr(e))\n",
    "# --- Core logic ---\n",
    "def get_last24h_and_averages(spark, db, batch_size=300, debug=False):\n",
    "    raw_data = read_all_users(db, batch_size=batch_size, debug=debug)\n",
    "    print(f\"‚úÖ Total records fetched: {len(raw_data)}\")\n",
    "\n",
    "    df = build_dataframe(spark, raw_data)\n",
    "    if df.isEmpty():\n",
    "        print(\"‚ÑπÔ∏è No data available.\")\n",
    "        return df, df\n",
    "\n",
    "    df_recent = df.filter(col(\"timestamp\") >= expr(\"current_timestamp() - INTERVAL 24 HOURS\"))\n",
    "    print(f\"üïí Records after 24h filter: {df_recent.count()}\")\n",
    "\n",
    "    df_avg = (\n",
    "        df_recent.groupBy(\"user_id\", \"metric_type\")\n",
    "        .agg(\n",
    "            # Conditional aggregation: use avg for heart_rate, sum for others\n",
    "            avg(when(col(\"metric_type\") == \"heart_rate\", col(\"value\"))).alias(\"heartbeat_avg\"),\n",
    "            _sum(when(col(\"metric_type\") != \"heart_rate\", col(\"value\"))).alias(\"others_sum\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"daily_value\",\n",
    "            when(col(\"metric_type\") == \"heart_rate\", col(\"heartbeat_avg\"))\n",
    "            .otherwise(col(\"others_sum\"))\n",
    "        )\n",
    "        .select(\"user_id\", \"metric_type\", \"daily_value\")\n",
    "        .orderBy(\"user_id\", \"metric_type\")\n",
    "    )\n",
    "\n",
    "    return df_recent, df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac7532-85ff-42e9-9d82-8d16f6245e8b",
   "metadata": {},
   "source": [
    "### excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92bf7329-58b3-4c21-a794-2c0e8ddcef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ get_user_ids: using list_documents(), found 34 users.\n",
      "üìã Found 34 user doc ids.\n",
      "Processing user: 1503960366\n",
      "Processing user: 1644430081\n",
      "Processing user: 1844505072\n",
      "Processing user: 1927972279\n",
      "Processing user: 2022484408\n",
      "Processing user: 2026352035\n",
      "Processing user: 2320127002\n",
      "Processing user: 2347167796\n",
      "Processing user: 2873212765\n",
      "Processing user: 3372868164\n",
      "Processing user: 3977333714\n",
      "Processing user: 4020332650\n",
      "Processing user: 4057192912\n",
      "Processing user: 4319703577\n",
      "Processing user: 4388161847\n",
      "Processing user: 4445114986\n",
      "Processing user: 4558609924\n",
      "Processing user: 4702921684\n",
      "Processing user: 5553957443\n",
      "Processing user: 5577150313\n",
      "Processing user: 6290855005\n",
      "Processing user: 6775888955\n",
      "Processing user: 6962181067\n",
      "Processing user: 7007744171\n",
      "Processing user: 7086361926\n",
      "Processing user: 8053475328\n",
      "Processing user: 8378563200\n",
      "Processing user: 8583815059\n",
      "Processing user: 8792009665\n",
      "Processing user: 8877689391\n",
      "Processing user: user_4020332650\n",
      "‚ÑπÔ∏è No subcollection docs for user user_4020332650.\n",
      "Processing user: user_5553957443\n",
      "‚ÑπÔ∏è No subcollection docs for user user_5553957443.\n",
      "Processing user: user_5577150313\n",
      "‚ÑπÔ∏è No subcollection docs for user user_5577150313.\n",
      "Processing user: y9G7aOFLbfs8DWDplgY8\n",
      "‚ÑπÔ∏è No subcollection docs for user y9G7aOFLbfs8DWDplgY8.\n",
      "‚úÖ Total records fetched: 12108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataFrame created with 12108 rows.\n",
      "üïí Records after 24h filter: 0\n"
     ]
    }
   ],
   "source": [
    "df_recent, df_daily_activity = get_last24h_and_averages(spark, db, batch_size=200, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a01a3-fb0d-48c2-9e86-882f712eaea4",
   "metadata": {},
   "source": [
    "## load daily meals df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd25265-2346-4467-ab9c-28eabd24cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_daily_meals( db, debug=False):\n",
    "    \"\"\"\n",
    "    Load monthly_plan documents for all users for target_date and return a Spark DataFrame.\n",
    "    - spark: SparkSession\n",
    "    - db: firestore.Client()\n",
    "    - target_date: datetime.date | datetime.datetime | 'YYYY-MM-DD' string\n",
    "    \"\"\"\n",
    "\n",
    "    user_ids = get_user_ids(db)\n",
    "    if debug:\n",
    "        print(f\"Found {len(user_ids)} users\")\n",
    "    \n",
    "    rows = []\n",
    "    for uid in user_ids:\n",
    "\n",
    "        docs = db.collection(f\"users/{uid}/daily_meals\").stream()\n",
    "        docs = [doc.to_dict() for doc in docs]\n",
    "        try:\n",
    "            for doc in docs:\n",
    "                rows.append(doc)\n",
    "        \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Query failed for {uid}: {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        if debug:\n",
    "            print(f\"‚ö†Ô∏è No meals found\")\n",
    "        empty_schema = StructType([\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema=empty_schema)\n",
    "\n",
    "    # infer schema dynamically from first document\n",
    "    sample = rows[0]\n",
    "    fields = []\n",
    "    for k, v in sample.items():\n",
    "        if k == \"user_id\":\n",
    "            fields.append(StructField(k, StringType(), True))\n",
    "        elif isinstance(v, bool):\n",
    "            fields.append(StructField(k, BooleanType(), True))\n",
    "        elif isinstance(v, (int, float)):\n",
    "            fields.append(StructField(k, DoubleType(), True))\n",
    "        else:\n",
    "            fields.append(StructField(k, StringType(), True))\n",
    "\n",
    "    schema = StructType(fields)\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "    return df\n",
    "df_daily_meals = load_daily_meals(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ef56d0-9f64-4c51-acb5-b39d401fa748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+---------+--------------+----------+------------------+------------------+----------+\n",
      "|          height_m|sport_available|meal_type|     food_name|      date|         weight_kg|          water_ml|   user_id|\n",
      "+------------------+---------------+---------+--------------+----------+------------------+------------------+----------+\n",
      "|1.7999999523162842|           true|    Lunch|      Broccoli|2025-11-28|              76.0| 607.1673583984375|4020332650|\n",
      "|1.8200000524520874|           true|Breakfast|     Chocolate|2025-11-04| 62.79999923706055|441.80242919921875|4020332650|\n",
      "|1.8200000524520874|           true|    Snack|        Salmon|2025-11-04| 62.79999923706055| 182.9405975341797|4020332650|\n",
      "|1.7999999523162842|           true|    Snack|      Broccoli|2025-11-28|              76.0| 298.7261047363281|4020332650|\n",
      "|1.8200000524520874|          false|    Lunch|        Grapes|2025-11-21|62.400001525878906| 604.0103759765625|4020332650|\n",
      "|1.7599999904632568|           true|Breakfast|        Quinoa|2025-11-08|  81.0999984741211| 373.5942687988281|4020332650|\n",
      "|1.7999999523162842|           true|   Dinner|        Potato|2025-11-05| 76.19999694824219| 699.2251586914062|4020332650|\n",
      "|1.8200000524520874|          false|    Lunch|         Pasta|2025-11-05|62.900001525878906| 424.0685729980469|4020332650|\n",
      "|1.7999999523162842|           true|   Dinner|  Orange Juice|2025-11-11| 76.69999694824219| 679.7382202148438|4020332650|\n",
      "|1.8200000524520874|           true|   Dinner|     Chocolate|2025-11-02|61.599998474121094| 520.5990600585938|4020332650|\n",
      "|1.8200000524520874|          false|    Snack|        Carrot|2025-11-05|62.900001525878906| 176.0579833984375|4020332650|\n",
      "|1.7899999618530273|          false|   Dinner|         Pasta|2025-11-10|62.099998474121094| 690.9309692382812|4020332650|\n",
      "|1.8200000524520874|          false|   Dinner|Chicken Breast|2025-11-01|              62.0| 561.8298950195312|4020332650|\n",
      "|1.7599999904632568|          false|    Snack|         Pasta|2025-11-28|  80.5999984741211| 150.5348358154297|4020332650|\n",
      "|1.7899999618530273|          false|Breakfast|         Water|2025-11-28|62.099998474121094| 412.0415344238281|4020332650|\n",
      "|1.7799999713897705|           true|    Snack|        Butter|2025-11-28|              79.5|166.13470458984375|4020332650|\n",
      "|1.7799999713897705|          false|Breakfast|          Rice|2025-11-20| 79.80000305175781| 311.5688171386719|4020332650|\n",
      "|1.7899999618530273|          false|    Lunch|        Grapes|2025-11-22| 62.29999923706055|  652.682373046875|4020332650|\n",
      "|1.7599999904632568|           true|Breakfast|         Apple|2025-11-17|  80.9000015258789| 320.5516052246094|4020332650|\n",
      "|1.8200000524520874|          false|Breakfast|        Cheese|2025-11-26|62.099998474121094| 275.4757385253906|4020332650|\n",
      "+------------------+---------------+---------+--------------+----------+------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_daily_meals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae0e4e-c616-4908-a8ff-5520714261da",
   "metadata": {},
   "source": [
    "## daily planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dd9de-a76d-41b6-a67f-fdcc177c6a5d",
   "metadata": {},
   "source": [
    "### join helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c580e2d8-eb77-4003-8a0d-5edb17c046cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType, DoubleType, BooleanType\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"fatigue-detector\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\"))\n",
    "    logger.addHandler(h)\n",
    "\n",
    "# ------------------ Utilities ------------------\n",
    "\n",
    "def _ensure_columns(df: DataFrame, cols_with_types: dict = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure the DataFrame contains the listed columns. If a column is missing,\n",
    "    create it with NULL and cast to the supplied type (or StringType by default).\n",
    "    cols_with_types: { \"col_name\": pyspark.sql.types.DataType, ... }\n",
    "    \"\"\"\n",
    "    if cols_with_types is None:\n",
    "        cols_with_types = {}\n",
    "    existing = set(df.columns)\n",
    "    out = df\n",
    "    for col_name, dtype in cols_with_types.items():\n",
    "        if col_name not in existing:\n",
    "            logger.info(\"Column missing, adding NULL column: %s\", col_name)\n",
    "            # add null column with cast to dtype if provided\n",
    "            cast_type = dtype if dtype is not None else StringType()\n",
    "            out = out.withColumn(col_name, F.lit(None).cast(cast_type))\n",
    "    return out\n",
    "\n",
    "# ------------------ Helpers ------------------\n",
    "\n",
    "def _find_user_id_col(df: DataFrame):\n",
    "    \"\"\"Return the name of the user-id-like column, or None.\"\"\"\n",
    "    candidates = [\"user_id\", \"userid\", \"UserID\", \"User_Id\", \"uid\", \"user\"]\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def _normalize_user_id_column(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Ensure there is a 'user_id' column that's trimmed and lowercased.\"\"\"\n",
    "    col_name = _find_user_id_col(df)\n",
    "    if col_name is None:\n",
    "        # If no column looks like a user id, add a NULL user_id (so joins won't crash)\n",
    "        logger.warning(\"No user-id-like column found in DataFrame. Adding NULL 'user_id' column. Columns: %s\", df.columns)\n",
    "        return df.withColumn(\"user_id\", F.lit(None).cast(StringType()))\n",
    "    # create/replace normalized 'user_id' string column\n",
    "    return df.withColumn(\"user_id\", F.lower(F.trim(F.col(col_name).cast(StringType()))))\n",
    "\n",
    "def _pivot_activity_if_needed(activity_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pivot long-format activity df into wide; leave as-is if already wide.\n",
    "    This version tolerates missing columns and adds NULL columns for expected metrics.\n",
    "    \"\"\"\n",
    "    cols = set(activity_df.columns)\n",
    "\n",
    "    # ensure at least user_id exists (or add it as NULL)\n",
    "    activity_df = _normalize_user_id_column(activity_df)\n",
    "\n",
    "    if \"avg_heart_rate\" in cols or \"total_steps\" in cols:\n",
    "        logger.info(\"Activity DF already wide-format.\")\n",
    "        return activity_df\n",
    "\n",
    "    # If it's long format, confirm metric_type/daily_value exist\n",
    "    if \"metric_type\" in cols and \"daily_value\" in cols:\n",
    "        logger.info(\"Pivoting long-format activity DF (metric_type/daily_value).\")\n",
    "        # pivot; use first value so missing metrics become NULL automatically\n",
    "        pivoted = (\n",
    "            activity_df\n",
    "            .groupBy(\"user_id\")\n",
    "            .pivot(\"metric_type\")\n",
    "            .agg(F.first(\"daily_value\"))\n",
    "        )\n",
    "        # rename common metrics if present\n",
    "        rename_map = {}\n",
    "        if \"heart_rate\" in pivoted.columns:\n",
    "            rename_map[\"heart_rate\"] = \"avg_heart_rate\"\n",
    "        if \"steps\" in pivoted.columns:\n",
    "            rename_map[\"steps\"] = \"total_steps\"\n",
    "        if \"calories\" in pivoted.columns:\n",
    "            rename_map[\"calories\"] = \"total_calories_burnt\"\n",
    "\n",
    "        out = pivoted\n",
    "        for old, new in rename_map.items():\n",
    "            if old in out.columns:\n",
    "                out = out.withColumnRenamed(old, new)\n",
    "\n",
    "        # Ensure canonical columns exist so downstream code won't fail\n",
    "        needed = {\n",
    "            \"avg_heart_rate\": DoubleType(),\n",
    "            \"total_steps\": DoubleType(),\n",
    "            \"total_calories_burnt\": DoubleType()\n",
    "        }\n",
    "        out = _ensure_columns(out, needed)\n",
    "        return out\n",
    "\n",
    "    logger.warning(\"Activity DF does not have expected columns for pivot (avg_heart_rate/metric_type/daily_value). Columns: %s\", activity_df.columns)\n",
    "    # As a fallback, ensure canonical columns exist with NULLs\n",
    "    fallback_needed = {\n",
    "        \"avg_heart_rate\": DoubleType(),\n",
    "        \"total_steps\": DoubleType(),\n",
    "        \"total_calories_burnt\": DoubleType()\n",
    "    }\n",
    "    return _ensure_columns(activity_df, fallback_needed)\n",
    "\n",
    "def _safe_rename_nutrition_cols(nutrition_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Rename nutrition columns with spaces/units into safe snake_case names.\n",
    "       If a source column is missing, do nothing for it (no error).\n",
    "    \"\"\"\n",
    "    rename_map = {\n",
    "        \"Calories (kcal)\": \"calories_kcal\",\n",
    "        \"Protein (g)\": \"protein_g\",\n",
    "        \"Carbohydrates (g)\": \"carbs_g\",\n",
    "        \"Fat (g)\": \"fat_g\",\n",
    "        \"Fiber (g)\": \"fiber_g\",\n",
    "        \"Sugars (g)\": \"sugars_g\",\n",
    "        \"Sodium (mg)\": \"sodium_mg\",\n",
    "        \"Cholesterol (mg)\": \"cholesterol_mg\",\n",
    "        \"Water_Intake (ml)\": \"water_ml\"\n",
    "    }\n",
    "    df = nutrition_df\n",
    "    for old, new in rename_map.items():\n",
    "        if old in df.columns:\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "        # also handle total_ prefixed variants safely\n",
    "        total_old = f\"total_{old}\"\n",
    "        total_new = f\"total_{new}\"\n",
    "        if total_old in df.columns:\n",
    "            df = df.withColumnRenamed(total_old, total_new)\n",
    "\n",
    "    # After renaming, ensure canonical nutrition columns exist with sensible types\n",
    "    ensure_map = {\n",
    "        \"calories_kcal\": DoubleType(),\n",
    "        \"protein_g\": DoubleType(),\n",
    "        \"carbs_g\": DoubleType(),\n",
    "        \"fat_g\": DoubleType(),\n",
    "        \"fiber_g\": DoubleType(),\n",
    "        \"sugars_g\": DoubleType(),\n",
    "        \"sodium_mg\": DoubleType(),\n",
    "        \"cholesterol_mg\": DoubleType(),\n",
    "        \"water_ml\": DoubleType()\n",
    "    }\n",
    "    df = _ensure_columns(df, ensure_map)\n",
    "    return df\n",
    "\n",
    "def _detect_calorie_and_water_cols(nutrition_df: DataFrame):\n",
    "    \"\"\"\n",
    "    Return (calories_col, water_col) names if present; otherwise None.\n",
    "    This will not raise if columns are missing.\n",
    "    \"\"\"\n",
    "    calories_candidates = [\"total_calories_kcal\", \"calories_kcal\", \"total_Calories (kcal)\", \"Calories (kcal)\"]\n",
    "    water_candidates = [\"total_water_ml\", \"water_ml\", \"total_Water_Intake (ml)\", \"Water_Intake (ml)\"]\n",
    "\n",
    "    calories_col = next((c for c in calories_candidates if c in nutrition_df.columns), None)\n",
    "    water_col = next((c for c in water_candidates if c in nutrition_df.columns), None)\n",
    "\n",
    "    if calories_col is None:\n",
    "        logger.info(\"No calories column detected. Candidates tried: %s\", calories_candidates)\n",
    "    if water_col is None:\n",
    "        logger.info(\"No water column detected. Candidates tried: %s\", water_candidates)\n",
    "\n",
    "    return calories_col, water_col\n",
    "def _compute_physiologic_factors(df: DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"total_steps\", F.coalesce(F.col(\"total_steps\"), F.lit(0)))\n",
    "    df = df.withColumn(\"avg_heart_rate\", F.coalesce(F.col(\"avg_heart_rate\"), F.lit(70)))\n",
    "    df = df.withColumn(\"total_calories_burnt\", F.coalesce(F.col(\"total_calories_burnt\"), F.lit(2000)))\n",
    "    df = df.withColumn(\"calories_intake\", F.coalesce(F.col(\"calories_intake\"), F.lit(0)))\n",
    "    df = df.withColumn(\"water_ml\", F.coalesce(F.col(\"water_ml\"), F.lit(0)))\n",
    "    df = df.withColumn(\"sleep_hours\", F.coalesce(F.col(\"sleep_hours\"), F.lit(7.0)))\n",
    "    df = df.withColumn(\"weight\", F.coalesce(F.col(\"weight\"), F.lit(70.0)))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"sleep_factor\",\n",
    "        F.when(F.col(\"sleep_hours\") < 5, 0.3)\n",
    "         .when(F.col(\"sleep_hours\") < 6, 0.5)\n",
    "         .when(F.col(\"sleep_hours\") < 7, 0.8)\n",
    "         .when(F.col(\"sleep_hours\") <= 8, 1.0)\n",
    "         .otherwise(0.9)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"activity_factor\",\n",
    "        F.when(F.col(\"total_steps\") < 3000, 0.4)\n",
    "         .when(F.col(\"total_steps\") < 6000, 0.7)\n",
    "         .when(F.col(\"total_steps\") <= 10000, 1.0)\n",
    "         .when(F.col(\"total_steps\") <= 12000, 0.8)\n",
    "         .otherwise(0.6)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"heart_factor\",\n",
    "        F.when(F.col(\"avg_heart_rate\") <= 60, 1.0)\n",
    "         .when(F.col(\"avg_heart_rate\") <= 75, 0.8)\n",
    "         .when(F.col(\"avg_heart_rate\") <= 85, 0.6)\n",
    "         .otherwise(0.4)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"energy_ratio\",\n",
    "        F.when(F.col(\"total_calories_burnt\") > 0,\n",
    "               F.col(\"calories_intake\") / F.col(\"total_calories_burnt\"))\n",
    "         .otherwise(F.lit(0.0))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"energy_factor\",\n",
    "        F.when(F.col(\"energy_ratio\") < 0.7, 0.5)\n",
    "         .when(F.col(\"energy_ratio\") < 0.9, 0.8)\n",
    "         .when(F.col(\"energy_ratio\") <= 1.1, 1.0)\n",
    "         .otherwise(0.9)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"hydration_factor\",\n",
    "        F.least(F.col(\"water_ml\") / (F.col(\"weight\") * F.lit(35.0)), F.lit(1.0))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"fatigue_score\",\n",
    "        0.35 * F.col(\"sleep_factor\") +\n",
    "        0.20 * F.col(\"activity_factor\") +\n",
    "        0.15 * F.col(\"heart_factor\") +\n",
    "        0.20 * F.col(\"energy_factor\") +\n",
    "        0.10 * F.col(\"hydration_factor\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"fatigue_level\",\n",
    "        F.when(F.col(\"fatigue_score\") < 0.4, \"Exhausted\")\n",
    "         .when(F.col(\"fatigue_score\") < 0.65, \"Tired\")\n",
    "         .when(F.col(\"fatigue_score\") < 0.85, \"Normal\")\n",
    "         .otherwise(\"Energetic\")\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664e74a-248f-4f42-83f5-a72ddee9ec43",
   "metadata": {},
   "source": [
    "### compute fatigue functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce503c7-aeee-4237-93f0-9b8326cacd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_daily_fatigue_with_diagnostics(raw_activity_df: DataFrame, raw_nutrition_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Detect fatigue per user, continuing even when data or columns are missing.\n",
    "    Returns DataFrame with fatigue metrics and diagnostic info.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize IDs early\n",
    "    activity_df = _normalize_user_id_column(raw_activity_df)\n",
    "    nutrition_df = _normalize_user_id_column(raw_nutrition_df)\n",
    "\n",
    "    logger.info(\"Activity columns: %s\", activity_df.columns)\n",
    "    logger.info(\"Nutrition columns: %s\", nutrition_df.columns)\n",
    "\n",
    "    # Safely count rows\n",
    "    try:\n",
    "        a_count, n_count = activity_df.count(), nutrition_df.count()\n",
    "        logger.info(\"Counts: activity=%d, nutrition=%d\", a_count, n_count)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Could not count DataFrames: %s\", e)\n",
    "        a_count = n_count = -1\n",
    "\n",
    "    # Sample user IDs\n",
    "    try:\n",
    "        a_sample = [r[\"user_id\"] for r in activity_df.select(\"user_id\").distinct().limit(10).collect()] if \"user_id\" in activity_df.columns else []\n",
    "        n_sample = [r[\"user_id\"] for r in nutrition_df.select(\"user_id\").distinct().limit(10).collect()] if \"user_id\" in nutrition_df.columns else []\n",
    "        logger.info(\"Sample user_ids activity=%s\", a_sample)\n",
    "        logger.info(\"Sample user_ids nutrition=%s\", n_sample)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Could not sample user_ids: %s\", e)\n",
    "\n",
    "    # Pivot and normalize activity safely\n",
    "    activity_pivoted = _pivot_activity_if_needed(activity_df)\n",
    "    activity_pivoted = _normalize_user_id_column(activity_pivoted)\n",
    "\n",
    "    # Rename nutrition safely and ensure expected cols exist\n",
    "    nutrition_safe = _safe_rename_nutrition_cols(nutrition_df)\n",
    "\n",
    "    # Detect columns dynamically\n",
    "    calories_col, water_col = _detect_calorie_and_water_cols(nutrition_safe)\n",
    "    logger.info(\"Detected columns: calories=%s, water=%s\", calories_col, water_col)\n",
    "\n",
    "    # Guarantee expected nutrition columns exist (even if empty)\n",
    "    expected_nutrition_cols = [\"sleep_hours\", \"weight\", \"height\"]\n",
    "    for colname in expected_nutrition_cols:\n",
    "        if colname not in nutrition_safe.columns:\n",
    "            logger.info(\"Missing column in nutrition, adding NULL: %s\", colname)\n",
    "            nutrition_safe = nutrition_safe.withColumn(colname, F.lit(None).cast(DoubleType()))\n",
    "\n",
    "    # Detect or fallback for date\n",
    "    has_a_date, has_n_date = \"date\" in activity_pivoted.columns, \"date\" in nutrition_safe.columns\n",
    "    if has_a_date and has_n_date:\n",
    "        date_expr = F.coalesce(F.col(\"a.date\"), F.col(\"n.date\")).alias(\"date\")\n",
    "    elif has_a_date:\n",
    "        date_expr = F.col(\"a.date\").alias(\"date\")\n",
    "    elif has_n_date:\n",
    "        date_expr = F.col(\"n.date\").alias(\"date\")\n",
    "    else:\n",
    "        logger.info(\"No date columns found ‚Äî using current_date as fallback\")\n",
    "        date_expr = F.current_date().alias(\"date\")\n",
    "\n",
    "    # Build select list safely\n",
    "    sel = [\n",
    "        F.col(\"a.user_id\").alias(\"user_id\"),\n",
    "        date_expr,\n",
    "        F.col(\"a.avg_heart_rate\") if \"avg_heart_rate\" in activity_pivoted.columns else F.lit(None).alias(\"avg_heart_rate\"),\n",
    "        F.col(\"a.total_steps\") if \"total_steps\" in activity_pivoted.columns else F.lit(None).alias(\"total_steps\"),\n",
    "        F.col(\"a.total_calories_burnt\") if \"total_calories_burnt\" in activity_pivoted.columns else F.lit(None).alias(\"total_calories_burnt\"),\n",
    "        (F.col(f\"n.{calories_col}\") if calories_col else F.lit(None)).alias(\"calories_intake\"),\n",
    "        F.col(\"n.sleep_hours\") if \"sleep_hours\" in nutrition_safe.columns else F.lit(None).alias(\"sleep_hours\"),\n",
    "        (F.col(\"n.weight\") if \"weight\" in nutrition_safe.columns else F.lit(None)).alias(\"weight\"),\n",
    "        (F.col(\"n.height\") if \"height\" in nutrition_safe.columns else F.lit(None)).alias(\"height\"),\n",
    "        (F.col(f\"n.{water_col}\") if water_col else F.lit(None)).alias(\"water_ml\"),\n",
    "    ]\n",
    "\n",
    "    # Try joining safely\n",
    "    try:\n",
    "        joined_inner = (\n",
    "            activity_pivoted.alias(\"a\")\n",
    "            .join(nutrition_safe.alias(\"n\"), on=\"user_id\", how=\"inner\")\n",
    "            .select(*sel)\n",
    "        )\n",
    "        joined_count = joined_inner.count()\n",
    "        logger.info(\"Inner join produced %d rows\", joined_count)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Join failed (%s). Falling back to left join.\", e)\n",
    "        joined_inner = activity_pivoted.alias(\"a\").join(nutrition_safe.alias(\"n\"), on=\"user_id\", how=\"left\").select(*sel)\n",
    "        joined_count = -1\n",
    "\n",
    "    if joined_count == 0:\n",
    "        logger.warning(\"Inner join empty ‚Äî diagnosing mismatch and using left join fallback.\")\n",
    "        try:\n",
    "            a_uids = set([r[\"user_id\"] for r in activity_pivoted.select(\"user_id\").distinct().collect()])\n",
    "            n_uids = set([r[\"user_id\"] for r in nutrition_safe.select(\"user_id\").distinct().collect()])\n",
    "            logger.info(\"Unique activity IDs (sample): %s\", list(a_uids)[:20])\n",
    "            logger.info(\"Unique nutrition IDs (sample): %s\", list(n_uids)[:20])\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Could not inspect user_id sets: %s\", e)\n",
    "\n",
    "        joined_left = (\n",
    "            activity_pivoted.alias(\"a\")\n",
    "            .join(nutrition_safe.alias(\"n\"), on=\"user_id\", how=\"left\")\n",
    "            .select(*sel)\n",
    "        )\n",
    "        logger.info(\"Left join row count: %d\", joined_left.count())\n",
    "        result = _compute_physiologic_factors(joined_left)\n",
    "        result = result.withColumn(\"join_diagnostic\", F.lit(\"inner_empty_fallback_left\"))\n",
    "        return result\n",
    "\n",
    "    # Normal path\n",
    "    result = _compute_physiologic_factors(joined_inner)\n",
    "    result = result.withColumn(\"join_diagnostic\", F.lit(\"inner_ok\"))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2d92a0-73dd-4143-9eb8-bcef9ae1b1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-14 20:26:21,929 - Activity columns: ['user_id', 'metric_type', 'daily_value']\n",
      "[INFO] 2025-11-14 20:26:21,931 - Nutrition columns: ['height_m', 'sport_available', 'meal_type', 'food_name', 'date', 'weight_kg', 'water_ml', 'user_id']\n",
      "[INFO] 2025-11-14 20:26:22,291 - Counts: activity=0, nutrition=2160\n",
      "[INFO] 2025-11-14 20:26:22,595 - Sample user_ids activity=[]\n",
      "[INFO] 2025-11-14 20:26:22,596 - Sample user_ids nutrition=['4020332650', '5553957443', '5577150313', 'user_4020332650', 'user_5553957443', 'user_5577150313']\n",
      "[INFO] 2025-11-14 20:26:22,611 - Pivoting long-format activity DF (metric_type/daily_value).\n",
      "[INFO] 2025-11-14 20:26:22,787 - Column missing, adding NULL column: avg_heart_rate\n",
      "[INFO] 2025-11-14 20:26:22,796 - Column missing, adding NULL column: total_steps\n",
      "[INFO] 2025-11-14 20:26:22,808 - Column missing, adding NULL column: total_calories_burnt\n",
      "[INFO] 2025-11-14 20:26:22,831 - Column missing, adding NULL column: calories_kcal\n",
      "[INFO] 2025-11-14 20:26:22,838 - Column missing, adding NULL column: protein_g\n",
      "[INFO] 2025-11-14 20:26:22,849 - Column missing, adding NULL column: carbs_g\n",
      "[INFO] 2025-11-14 20:26:22,859 - Column missing, adding NULL column: fat_g\n",
      "[INFO] 2025-11-14 20:26:22,868 - Column missing, adding NULL column: fiber_g\n",
      "[INFO] 2025-11-14 20:26:22,879 - Column missing, adding NULL column: sugars_g\n",
      "[INFO] 2025-11-14 20:26:22,894 - Column missing, adding NULL column: sodium_mg\n",
      "[INFO] 2025-11-14 20:26:22,907 - Column missing, adding NULL column: cholesterol_mg\n",
      "[INFO] 2025-11-14 20:26:22,921 - Detected columns: calories=calories_kcal, water=water_ml\n",
      "[INFO] 2025-11-14 20:26:22,921 - Missing column in nutrition, adding NULL: sleep_hours\n",
      "[INFO] 2025-11-14 20:26:22,937 - Missing column in nutrition, adding NULL: weight\n",
      "[INFO] 2025-11-14 20:26:22,949 - Missing column in nutrition, adding NULL: height\n",
      "[INFO] 2025-11-14 20:26:23,199 - Inner join produced 0 rows\n",
      "[WARNING] 2025-11-14 20:26:23,200 - Inner join empty ‚Äî diagnosing mismatch and using left join fallback.\n",
      "[INFO] 2025-11-14 20:26:23,504 - Unique activity IDs (sample): []\n",
      "[INFO] 2025-11-14 20:26:23,506 - Unique nutrition IDs (sample): ['5553957443', '5577150313', 'user_5577150313', 'user_4020332650', '4020332650', 'user_5553957443']\n",
      "[INFO] 2025-11-14 20:26:23,829 - Left join row count: 0\n"
     ]
    }
   ],
   "source": [
    "result_df = detect_daily_fatigue_with_diagnostics(df_daily_activity, df_daily_meals)\n",
    "fatigue_df = result_df.drop(\"join_diagnostic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d63d3-6041-4449-9611-fc7af5919c7e",
   "metadata": {},
   "source": [
    "## generate daily plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab1c33-1178-488e-bd39-d9ce17d0d766",
   "metadata": {},
   "source": [
    "### load the target plan of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d41d219-4bad-4c6c-ac3a-59571aca4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _to_date(raw):\n",
    "    if raw is None:\n",
    "        return None\n",
    "    if isinstance(raw, datetime.date) and not isinstance(raw, datetime.datetime):\n",
    "        return raw\n",
    "    if isinstance(raw, datetime.datetime):\n",
    "        return raw.date()\n",
    "    if isinstance(raw, str):\n",
    "        s = raw.strip()\n",
    "        try:\n",
    "            return datetime.date.fromisoformat(s)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return datetime.datetime.fromisoformat(s.replace(\"Z\", \"+00:00\")).date()\n",
    "            except Exception:\n",
    "                import re\n",
    "                m = re.search(r\"(\\d{4})[-/](\\d{2})[-/](\\d{2})\", s)\n",
    "                if m:\n",
    "                    y, mo, d = m.groups()\n",
    "                    try:\n",
    "                        return datetime.date(int(y), int(mo), int(d))\n",
    "                    except Exception:\n",
    "                        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_monthly_plan_for_date( db, target_date, debug=False):\n",
    "    \"\"\"\n",
    "    Load monthly_plan documents for all users for target_date and return a Spark DataFrame.\n",
    "    - spark: SparkSession\n",
    "    - db: firestore.Client()\n",
    "    - target_date: datetime.date | datetime.datetime | 'YYYY-MM-DD' string\n",
    "    \"\"\"\n",
    "    if isinstance(target_date, datetime.datetime):\n",
    "        target_date_str = target_date.date().isoformat()\n",
    "    elif isinstance(target_date, datetime.date):\n",
    "        target_date_str = target_date.isoformat()\n",
    "    else:\n",
    "        target_date_str = str(target_date).strip()\n",
    "\n",
    "    user_ids = get_user_ids(db)\n",
    "    if debug:\n",
    "        print(f\"Found {len(user_ids)} users\")\n",
    "    \n",
    "    rows = []\n",
    "    for uid in user_ids:\n",
    "\n",
    "        docs = db.collection(f\"users/{uid}/monthly_plan\").stream()\n",
    "        docs = [doc.to_dict() for doc in docs]\n",
    "        try:\n",
    "            for doc in docs:\n",
    "                doc[\"date\"] = target_date_str\n",
    "                rows.append(doc)\n",
    "        \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Query failed for {uid}: {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        if debug:\n",
    "            print(f\"‚ö†Ô∏è No monthly_plan found for {target_date_str}\")\n",
    "        empty_schema = StructType([\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "            StructField(\"date\", DateType(), True),\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema=empty_schema)\n",
    "\n",
    "    # infer schema dynamically from first document\n",
    "    sample = rows[0]\n",
    "    fields = []\n",
    "    for k, v in sample.items():\n",
    "        if k == \"user_id\":\n",
    "            fields.append(StructField(k, StringType(), True))\n",
    "        elif k == \"date\":\n",
    "            fields.append(StructField(k, StringType(), True))  # keep as string for consistency\n",
    "        elif isinstance(v, (int, float)):\n",
    "            fields.append(StructField(k, DoubleType(), True))\n",
    "        else:\n",
    "            fields.append(StructField(k, StringType(), True))\n",
    "\n",
    "    schema = StructType(fields)\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d25463d-6574-48f1-ad29-b244792a2012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 users\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['total_Calories (kcal)',\n",
       " 'sleep_hours',\n",
       " 'total_Sugars (g)',\n",
       " 'total_Sodium (mg)',\n",
       " 'total_Fat (g)',\n",
       " 'target_steps',\n",
       " 'total_Cholesterol (mg)',\n",
       " 'date',\n",
       " 'target_calories_burnt',\n",
       " 'total_Carbohydrates (g)',\n",
       " 'user_id',\n",
       " 'total_Protein (g)',\n",
       " 'total_Fiber (g)',\n",
       " 'total_Water_Intake (ml)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plans = load_monthly_plan_for_date(db, \"2025-11-12\", debug=True)\n",
    "# create Spark DF if you want:\n",
    "df_plans.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b711f-0218-4fab-8615-f80f746cb6b9",
   "metadata": {},
   "source": [
    "### load reference food and activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7703570a-39f8-486f-8914-44787b3540ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load datasets ---\n",
    "if not food_df:\n",
    "    food_df = spark.read.csv(\"./food_nutrition_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# --- 2. Define meal structure ---\n",
    "meal_structure = {\n",
    "    \"Breakfast\": [\"Grains\", \"Dairy\", \"Fruits\", \"Beverages\"],\n",
    "    \"Lunch\": [\"Meat\", \"Vegetables\", \"Grains\"],\n",
    "    \"Snack\": [\"Snacks\", \"Fruits\", \"Beverages\"],\n",
    "    \"Dinner\": [\"Meat\", \"Vegetables\", \"Grains\", \"Dairy\"]\n",
    "}\n",
    "\n",
    "# Broadcast static food data for performance\n",
    "food_broadcast = spark.sparkContext.broadcast(food_df.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "908d41f4-c1c1-430d-a8ca-128961fa91f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------+---------+------------+---------+\n",
      "|Activity                   |Calories_Burned|Category |Duration_min|Intensity|\n",
      "+---------------------------+---------------+---------+------------+---------+\n",
      "|Rest                       |0              |Tired    |30          |Very Low |\n",
      "|Gentle Yoga                |80             |Tired    |30          |Low      |\n",
      "|Light Walk                 |100            |Recovery |20          |Low      |\n",
      "|Stretching Routine         |50             |Recovery |15          |Low      |\n",
      "|Brisk Walk                 |150            |Normal   |30          |Moderate |\n",
      "|Bodyweight Strength        |200            |Normal   |30          |Moderate |\n",
      "|Cycling (Easy)             |180            |Normal   |30          |Moderate |\n",
      "|HIIT Circuit               |300            |Energetic|20          |High     |\n",
      "|Running / Jogging          |350            |Energetic|30          |High     |\n",
      "|Strength Training (Weights)|400            |Energetic|40          |High     |\n",
      "|Spin Class / Cardio        |380            |Energetic|30          |High     |\n",
      "+---------------------------+---------------+---------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "activity_data = [\n",
    "    # --- Recovery / Tired states ---\n",
    "    {\"Activity\": \"Rest\", \"Category\": \"Tired\", \"Duration_min\": 30, \"Calories_Burned\": 0, \"Intensity\": \"Very Low\"},\n",
    "    {\"Activity\": \"Gentle Yoga\", \"Category\": \"Tired\", \"Duration_min\": 30, \"Calories_Burned\": 80, \"Intensity\": \"Low\"},\n",
    "    {\"Activity\": \"Light Walk\", \"Category\": \"Recovery\", \"Duration_min\": 20, \"Calories_Burned\": 100, \"Intensity\": \"Low\"},\n",
    "    {\"Activity\": \"Stretching Routine\", \"Category\": \"Recovery\", \"Duration_min\": 15, \"Calories_Burned\": 50, \"Intensity\": \"Low\"},\n",
    "\n",
    "    # --- Normal state ---\n",
    "    {\"Activity\": \"Brisk Walk\", \"Category\": \"Normal\", \"Duration_min\": 30, \"Calories_Burned\": 150, \"Intensity\": \"Moderate\"},\n",
    "    {\"Activity\": \"Bodyweight Strength\", \"Category\": \"Normal\", \"Duration_min\": 30, \"Calories_Burned\": 200, \"Intensity\": \"Moderate\"},\n",
    "    {\"Activity\": \"Cycling (Easy)\", \"Category\": \"Normal\", \"Duration_min\": 30, \"Calories_Burned\": 180, \"Intensity\": \"Moderate\"},\n",
    "\n",
    "    # --- Energetic state ---\n",
    "    {\"Activity\": \"HIIT Circuit\", \"Category\": \"Energetic\", \"Duration_min\": 20, \"Calories_Burned\": 300, \"Intensity\": \"High\"},\n",
    "    {\"Activity\": \"Running / Jogging\", \"Category\": \"Energetic\", \"Duration_min\": 30, \"Calories_Burned\": 350, \"Intensity\": \"High\"},\n",
    "    {\"Activity\": \"Strength Training (Weights)\", \"Category\": \"Energetic\", \"Duration_min\": 40, \"Calories_Burned\": 400, \"Intensity\": \"High\"},\n",
    "    {\"Activity\": \"Spin Class / Cardio\", \"Category\": \"Energetic\", \"Duration_min\": 30, \"Calories_Burned\": 380, \"Intensity\": \"High\"},\n",
    "]\n",
    "\n",
    "activity_df = spark.createDataFrame(activity_data)\n",
    "activity_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561033e-898a-4218-9dd1-bda3b5663687",
   "metadata": {},
   "source": [
    "### meals planner functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "528336e0-fe97-4235-8679-0dfc12094577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def adjust_daily_targets_pro(row):\n",
    "    \"\"\"\n",
    "    Professional-grade adaptive target adjustment logic.\n",
    "    Uses fatigue, sleep, hydration, activity, and energy balance to optimize user plans.\n",
    "    \"\"\"\n",
    "    # --- Extract key inputs ---\n",
    "    fatigue = row.fatigue_score if row.fatigue_score is not None else 0.8\n",
    "    sleep = row.sleep_hours\n",
    "    intake = row.calories_intake\n",
    "    burnt = row.total_calories_burnt\n",
    "    water = row.water_ml\n",
    "    weight = row.weight\n",
    "    height = row.height\n",
    "    activity = row.total_steps\n",
    "\n",
    "    # --- Derived metrics ---\n",
    "    energy_balance = (intake - burnt) / max(intake, 1)\n",
    "    hydration_ratio = water / max(row[\"total_Water_Intake (ml)\"], 1)\n",
    "    sleep_deficit = (row[\"sleep_hours\"] - sleep) / max(row[\"sleep_hours\"], 1)\n",
    "    tired = fatigue < 0.7\n",
    "    overreached = fatigue < 0.5 and sleep_deficit > 0.2\n",
    "    energetic = fatigue > 0.9 and sleep > row[\"sleep_hours\"]\n",
    "\n",
    "    # --- Base daily targets ---\n",
    "    targets = {\n",
    "        \"total_Calories (kcal)\": row[\"total_Calories (kcal)\"],\n",
    "        \"total_Protein (g)\": row[\"total_Protein (g)\"],\n",
    "        \"total_Carbohydrates (g)\": row[\"total_Carbohydrates (g)\"],\n",
    "        \"total_Fat (g)\": row[\"total_Fat (g)\"],\n",
    "        \"total_Water_Intake (ml)\": row[\"total_Water_Intake (ml)\"],\n",
    "        \"sleep_hours\": row[\"sleep_hours\"],\n",
    "        \"target_steps\": row[\"target_steps\"],\n",
    "        \"target_calories_burnt\": row[\"target_calories_burnt\"],\n",
    "    }\n",
    "\n",
    "    # --- Adaptive adjustment rules ---\n",
    "    if overreached:\n",
    "        # Deep recovery mode\n",
    "        adj = {\n",
    "            \"total_Calories (kcal)\": 0.95,\n",
    "            \"total_Protein (g)\": 1.15,\n",
    "            \"total_Fat (g)\": 0.9,\n",
    "            \"target_steps\": 0.75,\n",
    "            \"target_calories_burnt\": 0.75,\n",
    "            \"sleep_hours\": +1.0,\n",
    "            \"total_Water_Intake (ml)\": 1.2\n",
    "        }\n",
    "        state = \"Recovery\"\n",
    "\n",
    "    elif tired:\n",
    "        # Light fatigue ‚Üí promote repair\n",
    "        adj = {\n",
    "            \"total_Calories (kcal)\": 0.9,\n",
    "            \"total_Protein (g)\": 1.1,\n",
    "            \"total_Fat (g)\": 0.85,\n",
    "            \"target_steps\": 0.85,\n",
    "            \"target_calories_burnt\": 0.85,\n",
    "            \"sleep_hours\": +0.5,\n",
    "            \"total_Water_Intake (ml)\": 1.1\n",
    "        }\n",
    "        state = \"Tired\"\n",
    "\n",
    "    elif energetic:\n",
    "        # Strong recovery ‚Üí allow progression\n",
    "        adj = {\n",
    "            \"total_Calories (kcal)\": 1.1,\n",
    "            \"total_Protein (g)\": 1.05,\n",
    "            \"total_Carbohydrates (g)\": 1.1,\n",
    "            \"target_steps\": 1.15,\n",
    "            \"target_calories_burnt\": 1.15,\n",
    "            \"total_Water_Intake (ml)\": 1.05\n",
    "        }\n",
    "        state = \"Energetic\"\n",
    "\n",
    "    else:\n",
    "        # Normal balance\n",
    "        adj = {k: 1.0 for k in targets}\n",
    "        state = \"Normal\"\n",
    "\n",
    "    # --- Fine-tune hydration & energy ---\n",
    "    # Correct for hydration deficiency or excess\n",
    "    if hydration_ratio < 0.9:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.15\n",
    "    elif hydration_ratio > 1.2:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 0.95\n",
    "\n",
    "    # Adjust energy targets based on balance\n",
    "    if energy_balance < -0.15:  # calorie deficit too large\n",
    "        targets[\"total_Calories (kcal)\"] *= 1.05\n",
    "    elif energy_balance > 0.15:  # surplus too large\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.95\n",
    "\n",
    "    # Apply scaling\n",
    "    for k, v in adj.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            if v > 1:\n",
    "                targets[k] *= v\n",
    "            elif v < 1:\n",
    "                targets[k] *= v\n",
    "            elif isinstance(v, (int, float)) and \"+\" in str(v):\n",
    "                targets[k] += v\n",
    "\n",
    "    return targets, state\n",
    "\n",
    "\n",
    "# --- 2. Meal Generation ---\n",
    "meal_structure = {\n",
    "    \"Breakfast\": [\"Grains\", \"Dairy\", \"Fruits\", \"Beverages\"],\n",
    "    \"Lunch\": [\"Meat\", \"Vegetables\", \"Grains\"],\n",
    "    \"Snack\": [\"Snacks\", \"Fruits\", \"Beverages\"],\n",
    "    \"Dinner\": [\"Meat\", \"Vegetables\", \"Grains\", \"Dairy\"]\n",
    "}\n",
    "\n",
    "def generate_plan_with_activity(row):\n",
    "    \"\"\"\n",
    "    Generate a full daily plan including meals and recommended activity.\n",
    "    Activity is chosen based on user's fatigue/recovery state and explained.\n",
    "    \"\"\"\n",
    "    meals = generate_explainable_plan(row)  # existing meal plan function\n",
    "\n",
    "    # Filter activities matching user's plan_state\n",
    "    activities = [a for a in activity_data.value if a[\"Category\"] == row.plan_state]\n",
    "    \n",
    "    # Fallback to Rest if none found\n",
    "    if activities:\n",
    "        chosen_activity = random.choice(activities)\n",
    "    else:\n",
    "        chosen_activity = {\n",
    "            \"Activity\": \"Rest\",\n",
    "            \"Category\": row.plan_state,\n",
    "            \"Duration_min\": 30,\n",
    "            \"Calories_Burned\": 0,\n",
    "            \"Intensity\": \"Very Low\"\n",
    "        }\n",
    "\n",
    "    # Append activity as a special \"meal/activity\" row\n",
    "    meals.append((\n",
    "        row.user_id,\n",
    "        row.date,\n",
    "        \"Activity\",                      # Treat as a meal type for partitioning\n",
    "        chosen_activity[\"Activity\"],\n",
    "        chosen_activity[\"Category\"],\n",
    "        float(chosen_activity[\"Calories_Burned\"]),  # calories burned\n",
    "        0.0, 0.0, 0.0,                    # no macronutrients\n",
    "        row.plan_state,\n",
    "        f\"Activity assigned based on state {row.plan_state}, duration {chosen_activity.get('Duration_min', 30)} min, intensity {chosen_activity.get('Intensity', 'Low')}\",\n",
    "        float(row.fatigue_score),\n",
    "        float(row.sleep_hours),\n",
    "        float(row.total_steps),\n",
    "        float(row.total_calories_burnt),\n",
    "        float(row.calories_intake),\n",
    "        float(row.total_Water_Intake),\n",
    "        0.0, 0.0\n",
    "    ))\n",
    "\n",
    "    return meals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70510f7c-0ba5-45de-9084-050edf0080af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. Final explainable plan schema\n",
    "# ----------------------------------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"meal\", StringType()),\n",
    "    StructField(\"food_item\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"Calories\", DoubleType()),\n",
    "    StructField(\"Protein\", DoubleType()),\n",
    "    StructField(\"Carbs\", DoubleType()),\n",
    "    StructField(\"Fat\", DoubleType()),\n",
    "    StructField(\"plan_state\", StringType()),          # Recovery / Tired / Energetic / Normal\n",
    "    StructField(\"reasoning\", StringType()),           # Human-readable explanation\n",
    "    StructField(\"fatigue_score\", DoubleType()),\n",
    "    StructField(\"sleep_hours\", DoubleType()),\n",
    "    StructField(\"total_steps\", DoubleType()),\n",
    "    StructField(\"total_calories_burnt\", DoubleType()),\n",
    "    StructField(\"calories_intake\", DoubleType()),\n",
    "    StructField(\"total_Water_Intake (ml)\", DoubleType()),\n",
    "    StructField(\"energy_balance\", DoubleType()),\n",
    "    StructField(\"hydration_ratio\", DoubleType())\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Adaptive scaling logic with explanation\n",
    "# ----------------------------------------------------\n",
    "def adjust_daily_targets_explainable(row):\n",
    "    # --- Safe getters ---\n",
    "    def g(x, default=0):\n",
    "        return x if x is not None else default\n",
    "\n",
    "    fatigue = g(row.fatigue_score, 0.8)\n",
    "    sleep = g(row.sleep_hours, 0)\n",
    "    intake = g(row.calories_intake, 0)\n",
    "    burnt = g(row.total_calories_burnt, 0)\n",
    "    water = g(row.water_ml, 0)\n",
    "    steps = g(row.total_steps, 0)\n",
    "\n",
    "    base_sleep = g(row[\"sleep_hours\"], sleep)\n",
    "    base_water = g(row[\"total_Water_Intake (ml)\"], water)\n",
    "    base_calories = g(row[\"total_Calories (kcal)\"], intake)\n",
    "\n",
    "    # --- Derived safely ---\n",
    "    energy_balance = (intake - burnt) / max(intake, 1)\n",
    "    hydration_ratio = water / max(base_water, 1)\n",
    "\n",
    "    # You previously had: (row[\"sleep_hours\"] - sleep)\n",
    "    # But row[\"sleep_hours\"] == sleep ‚Üí always 0 or None\n",
    "    # Meaning this metric was logically incorrect.\n",
    "    sleep_deficit = max(0, (8 - sleep) / 8)   # Fix: deficit relative to ideal sleep\n",
    "\n",
    "    # --- Fatigue logic ---\n",
    "    tired = fatigue < 0.7\n",
    "    overreached = fatigue < 0.5 and sleep_deficit > 0.2\n",
    "    energetic = fatigue > 0.9 and sleep > 7\n",
    "\n",
    "    # --- Initialize targets ---\n",
    "    targets = {\n",
    "        \"total_Calories (kcal)\": base_calories,\n",
    "        \"total_Protein (g)\": g(row[\"total_Protein (g)\"], 0),\n",
    "        \"total_Carbohydrates (g)\": g(row[\"total_Carbohydrates (g)\"], 0),\n",
    "        \"total_Fat (g)\": g(row[\"total_Fat (g)\"], 0),\n",
    "        \"total_Water_Intake (ml)\": base_water,\n",
    "        \"sleep_hours\": base_sleep,\n",
    "        \"target_steps\": g(row[\"target_steps\"], steps),\n",
    "        \"target_calories_burnt\": g(row[\"target_calories_burnt\"], burnt),\n",
    "    }\n",
    "\n",
    "    reasoning = []\n",
    "\n",
    "    # --- State logic unchanged ---\n",
    "    if overreached:\n",
    "        state = \"Recovery\"\n",
    "        reasoning.append(\"Detected signs of overreaching: low fatigue score and sleep deficit.\")\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.95\n",
    "        targets[\"total_Protein (g)\"] *= 1.15\n",
    "        targets[\"total_Fat (g)\"] *= 0.9\n",
    "        targets[\"target_steps\"] *= 0.75\n",
    "        targets[\"target_calories_burnt\"] *= 0.75\n",
    "        targets[\"sleep_hours\"] += 1.0\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.2\n",
    "\n",
    "    elif tired:\n",
    "        state = \"Tired\"\n",
    "        reasoning.append(\"Mild fatigue detected ‚Äî focusing on recovery and protein repair.\")\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.9\n",
    "        targets[\"total_Protein (g)\"] *= 1.1\n",
    "        targets[\"total_Fat (g)\"] *= 0.85\n",
    "        targets[\"target_steps\"] *= 0.85\n",
    "        targets[\"target_calories_burnt\"] *= 0.85\n",
    "        targets[\"sleep_hours\"] += 0.5\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.1\n",
    "\n",
    "    elif energetic:\n",
    "        state = \"Energetic\"\n",
    "        reasoning.append(\"High recovery ‚Äî increasing load slightly for progression.\")\n",
    "        targets[\"total_Calories (kcal)\"] *= 1.1\n",
    "        targets[\"total_Protein (g)\"] *= 1.05\n",
    "        targets[\"total_Carbohydrates (g)\"] *= 1.1\n",
    "        targets[\"target_steps\"] *= 1.15\n",
    "        targets[\"target_calories_burnt\"] *= 1.15\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.05\n",
    "\n",
    "    else:\n",
    "        state = \"Normal\"\n",
    "        reasoning.append(\"Balanced state ‚Äî maintaining plan at normal levels.\")\n",
    "\n",
    "    # --- Fine-tune ---\n",
    "    if energy_balance < -0.15:\n",
    "        targets[\"total_Calories (kcal)\"] *= 1.05\n",
    "        reasoning.append(\"Caloric deficit detected ‚Äî slightly increasing intake.\")\n",
    "    elif energy_balance > 0.15:\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.95\n",
    "        reasoning.append(\"Caloric surplus detected ‚Äî moderating intake.\")\n",
    "\n",
    "    if hydration_ratio < 0.9:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.15\n",
    "        reasoning.append(\"Low hydration ‚Äî increasing water intake.\")\n",
    "    elif hydration_ratio > 1.2:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 0.95\n",
    "        reasoning.append(\"High hydration ‚Äî lowering water target slightly.\")\n",
    "\n",
    "    return targets, state, \"; \".join(reasoning), energy_balance, hydration_ratio\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Adaptive meal plan generator\n",
    "# ----------------------------------------------------\n",
    "meal_structure = {\n",
    "    \"Breakfast\": [\"Grains\", \"Dairy\", \"Fruits\", \"Beverages\"],\n",
    "    \"Lunch\": [\"Meat\", \"Vegetables\", \"Grains\"],\n",
    "    \"Snack\": [\"Snacks\", \"Fruits\", \"Beverages\"],\n",
    "    \"Dinner\": [\"Meat\", \"Vegetables\", \"Grains\", \"Dairy\"]\n",
    "}\n",
    "def generate_explainable_plan(row):\n",
    "    global food_data\n",
    "\n",
    "    def safe_float(x):\n",
    "        return float(x) if x is not None else None\n",
    "\n",
    "    targets, state, reasoning, energy_balance, hydration_ratio = adjust_daily_targets_explainable(row)\n",
    "    meals = []\n",
    "\n",
    "    # Skip entire day if key targets are missing\n",
    "    if row.fatigue_score is None or row.sleep_hours is None:\n",
    "        return []\n",
    "\n",
    "    for meal, categories in meal_structure.items():\n",
    "        if state in [\"Tired\", \"Recovery\"] and meal in [\"Lunch\", \"Dinner\"]:\n",
    "            categories = [c for c in categories if c not in [\"Snacks\", \"Fried\", \"HighFat\"]]\n",
    "\n",
    "        allowed = [f for f in food_data.value if f[\"Category\"] in categories]\n",
    "        if not allowed:\n",
    "            continue\n",
    "\n",
    "        chosen = random.sample(allowed, min(3, len(allowed)))\n",
    "\n",
    "        for f in chosen:\n",
    "            cal = safe_float(f.get(\"Calories (kcal)\"))\n",
    "            prot = safe_float(f.get(\"Protein (g)\"))\n",
    "            carbs = safe_float(f.get(\"Carbohydrates (g)\"))\n",
    "            fat = safe_float(f.get(\"Fat (g)\"))\n",
    "\n",
    "            meals.append((\n",
    "                row.user_id,\n",
    "                row.date,\n",
    "                meal,\n",
    "                f[\"Food_Item\"],\n",
    "                f[\"Category\"],\n",
    "                cal,\n",
    "                prot,\n",
    "                carbs,\n",
    "                fat,\n",
    "                state,\n",
    "                reasoning,\n",
    "                safe_float(row.fatigue_score),\n",
    "                safe_float(row.sleep_hours),\n",
    "                safe_float(row.total_steps),\n",
    "                safe_float(row.total_calories_burnt),\n",
    "                safe_float(row.calories_intake),\n",
    "                safe_float(targets.get(\"total_Water_Intake (ml)\")),\n",
    "                safe_float(energy_balance),\n",
    "                safe_float(hydration_ratio)\n",
    "            ))\n",
    "    return meals\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Merge daily targets and fatigue data\n",
    "# ----------------------------------------------------\n",
    "daily_df = df_plans.join(fatigue_df, on=[\"user_id\", \"date\"], how=\"left\")\n",
    "food_data = spark.sparkContext.broadcast(food_df.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93e3df56-3865-4719-a3e8-8684986aed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+----------+--------+-----+-----------+\n",
      "|user_id|TotalCalories|TotalProtein|TotalCarbs|TotalFat|State|Explanation|\n",
      "+-------+-------------+------------+----------+--------+-----+-----------+\n",
      "+-------+-------------+------------+----------+--------+-----+-----------+\n",
      "\n",
      "+-------+----+----+---------+--------+--------+-------+-----+---+----------+---------+-------------+-----------+-----------+--------------------+---------------+-----------------------+--------------+---------------+\n",
      "|user_id|date|meal|food_item|category|Calories|Protein|Carbs|Fat|plan_state|reasoning|fatigue_score|sleep_hours|total_steps|total_calories_burnt|calories_intake|total_Water_Intake (ml)|energy_balance|hydration_ratio|\n",
      "+-------+----+----+---------+--------+--------+-------+-----+---+----------+---------+-------------+-----------+-----------+--------------------+---------------+-----------------------+--------------+---------------+\n",
      "+-------+----+----+---------+--------+--------+-------+-----+---+----------+---------+-------------+-----------+-----------+--------------------+---------------+-----------------------+--------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# 5. Generate adaptive explainable plans\n",
    "# ----------------------------------------------------\n",
    "all_plans = (\n",
    "    daily_df.rdd.flatMap(generate_explainable_plan)\n",
    "    .toDF(schema)\n",
    ")\n",
    "# ----------------------------------------------------\n",
    "# 6. User summaries\n",
    "# ----------------------------------------------------\n",
    "summary_df = (\n",
    "    all_plans.groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.sum(\"Calories\").alias(\"TotalCalories\"),\n",
    "        F.sum(\"Protein\").alias(\"TotalProtein\"),\n",
    "        F.sum(\"Carbs\").alias(\"TotalCarbs\"),\n",
    "        F.sum(\"Fat\").alias(\"TotalFat\"),\n",
    "        F.first(\"plan_state\").alias(\"State\"),\n",
    "        F.first(\"reasoning\").alias(\"Explanation\")\n",
    "    )\n",
    ")\n",
    "summary_df.show(10)\n",
    "summary_df.columns\n",
    "\n",
    "all_plans.show(10)\n",
    "all_plans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63e861cc-799e-4a0a-8f94-fae00dccf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import decimal\n",
    "import json\n",
    "from google.cloud import firestore\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, DateType\n",
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Logging Setup\n",
    "# ----------------------------\n",
    "fs_logger = logging.getLogger(\"firestore-writer\")\n",
    "fs_logger.setLevel(logging.INFO)\n",
    "if not fs_logger.hasHandlers():\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\"))\n",
    "    fs_logger.addHandler(h)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Helper Functions\n",
    "# ----------------------------\n",
    "def _serialize_value(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (datetime.datetime, datetime.date)):\n",
    "        return v.isoformat()\n",
    "    if isinstance(v, decimal.Decimal):\n",
    "        return float(v)\n",
    "    if isinstance(v, (bytes, bytearray)):\n",
    "        try:\n",
    "            return v.decode()\n",
    "        except Exception:\n",
    "            return str(v)\n",
    "    try:\n",
    "        json.dumps(v)\n",
    "        return v\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "\n",
    "def _commit_with_retries(batch_obj, max_retries=3, base_backoff=0.5):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch_obj.commit()\n",
    "            return\n",
    "        except (GoogleAPICallError, RetryError, IOError) as e:\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                fs_logger.exception(\"Firestore commit failed after %d attempts\", attempt - 1)\n",
    "                raise\n",
    "            backoff = base_backoff * (2 ** (attempt - 1))\n",
    "            fs_logger.warning(\n",
    "                \"Transient error committing firestore batch (attempt %d). Backing off %.2fs. Error: %s\",\n",
    "                attempt, backoff, str(e)\n",
    "            )\n",
    "            time.sleep(backoff)\n",
    "\n",
    "def _throttle():\n",
    "    nonlocal writes_this_minute, minute_window_start\n",
    "\n",
    "    now = time.time()\n",
    "    if now - minute_window_start >= 60:\n",
    "        writes_this_minute = 0\n",
    "        minute_window_start = now\n",
    "\n",
    "    if writes_this_minute >= max_writes_per_minute:\n",
    "        sleep_for = 60 - (now - minute_window_start)\n",
    "        fs_logger.warning(f\"Rate limit reached. Sleeping {sleep_for:.1f}s to stay under/min limit.\")\n",
    "        time.sleep(sleep_for)\n",
    "        writes_this_minute = 0\n",
    "        minute_window_start = time.time()\n",
    "\n",
    "def make_firestore_writer(\n",
    "    collection_name,\n",
    "    firestore_client,\n",
    "    batch_size=500,\n",
    "    max_retries=3,\n",
    "    max_writes_per_run=20000,     # Protect daily free-tier\n",
    "    max_writes_per_minute=500     # Optional rate limit\n",
    "):\n",
    "    writes_this_run = 0\n",
    "    writes_this_minute = 0\n",
    "    minute_window_start = time.time()\n",
    "\n",
    "    def write_batch_to_firestore(batch_df, epoch_id=None):\n",
    "        rows = batch_df.count()\n",
    "        if not rows:\n",
    "            fs_logger.info(\"[epoch %s] empty, skipping collection=%s\", str(epoch_id), collection_name)\n",
    "            return\n",
    "\n",
    "        fs_logger.info(\"[epoch %s] writing %s rows to Firestore collection '%s'\",\n",
    "                       str(epoch_id), rows, collection_name)\n",
    "\n",
    "        docs_written = 0\n",
    "        ops_in_current_batch = 0\n",
    "        fs_batch = firestore_client.batch()\n",
    "\n",
    "        for row in batch_df.toLocalIterator():\n",
    "        \n",
    "            if writes_this_run >= max_writes_per_run:\n",
    "                fs_logger.warning(\n",
    "                    f\"[epoch {epoch_id}] Stopped early ‚Äî reached max_writes_per_run={max_writes_per_run}\"\n",
    "                )\n",
    "                break\n",
    "        \n",
    "            _throttle()\n",
    "        \n",
    "            data = row.asDict(recursive=True)\n",
    "            user_id = data.get(\"user_id\")\n",
    "        \n",
    "            if not user_id:\n",
    "                fs_logger.warning(\"[epoch %s] skipping row without user_id\", str(epoch_id))\n",
    "                continue\n",
    "        \n",
    "            for k, v in list(data.items()):\n",
    "                data[k] = _serialize_value(v)\n",
    "        \n",
    "            doc_ref = (\n",
    "                firestore_client.collection(\"users\")\n",
    "                .document(str(user_id))\n",
    "                .collection(collection_name)\n",
    "                .document()\n",
    "            )\n",
    "        \n",
    "            fs_batch.set(doc_ref, data)\n",
    "            ops_in_current_batch += 1\n",
    "            writes_this_run += 1\n",
    "            writes_this_minute += 1\n",
    "        \n",
    "            if ops_in_current_batch >= batch_size:\n",
    "                _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "                ops_in_current_batch = 0\n",
    "                fs_batch = firestore_client.batch()\n",
    "\n",
    "        if ops_in_current_batch > 0:\n",
    "            _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "            docs_written += ops_in_current_batch\n",
    "\n",
    "        fs_logger.info(\"[epoch %s] wrote %d docs under users/*/%s/\",\n",
    "                       str(epoch_id), docs_written, collection_name)\n",
    "    return write_batch_to_firestore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b578771-09f0-4b5e-8508-2eff073a9be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-12 20:18:53,404 - [epoch None] writing 64800 rows to Firestore collection 'daily_plan'\n",
      "[WARNING] 2025-11-12 20:22:44,274 - Transient error committing firestore batch (attempt 1). Backing off 0.50s. Error: Timeout of 60.0s exceeded, last exception: 429 Quota exceeded.\n",
      "[WARNING] 2025-11-12 20:23:31,285 - Transient error committing firestore batch (attempt 2). Backing off 1.00s. Error: Timeout of 60.0s exceeded, last exception: 429 Quota exceeded.\n",
      "[WARNING] 2025-11-12 20:24:33,002 - Transient error committing firestore batch (attempt 3). Backing off 2.00s. Error: Timeout of 60.0s exceeded, last exception: 429 Quota exceeded.\n",
      "[ERROR] 2025-11-12 20:25:27,986 - Firestore commit failed after 3 attempts\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/grpc_helpers.py\", line 75, in error_remapped_callable\n",
      "    return callable_(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/grpc/_interceptor.py\", line 277, in __call__\n",
      "    response, ignored_call = self._with_call(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/grpc/_interceptor.py\", line 332, in _with_call\n",
      "    return call.result(), call\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/grpc/_channel.py\", line 440, in result\n",
      "    raise self\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/grpc/_interceptor.py\", line 315, in continuation\n",
      "    response, call = self._thunk(new_method).with_call(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/grpc/_channel.py\", line 1198, in with_call\n",
      "    return _end_unary_response_blocking(state, call, True, None)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
      "\tdetails = \"Quota exceeded.\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.200.234:443 {grpc_message:\"Quota exceeded.\", grpc_status:8, created_time:\"2025-11-12T20:25:27.986433911+00:00\"}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_51/1130174123.py\", line 49, in _commit_with_retries\n",
      "    batch_obj.commit()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/firestore_v1/batch.py\", line 61, in commit\n",
      "    commit_response = self._client._firestore_api.commit(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/firestore_v1/services/firestore/client.py\", line 1431, in commit\n",
      "    response = rpc(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_base.py\", line 229, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "google.api_core.exceptions.RetryError: Timeout of 60.0s exceeded, last exception: 429 Quota exceeded.\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "Timeout of 60.0s exceeded, last exception: 429 Quota exceeded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/grpc_helpers.py:75\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1192\u001b[0m (\n\u001b[1;32m   1193\u001b[0m     state,\n\u001b[1;32m   1194\u001b[0m     call,\n\u001b[1;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1197\u001b[0m )\n\u001b[0;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.200.234:443 {grpc_message:\"Quota exceeded.\", grpc_status:8, created_time:\"2025-11-12T20:25:27.986433911+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/grpc_helpers.py:77\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Quota exceeded.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m writer \u001b[38;5;241m=\u001b[39m make_firestore_writer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily_plan\u001b[39m\u001b[38;5;124m\"\u001b[39m, db)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Run the batch write\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mwriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_plans\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 100\u001b[0m, in \u001b[0;36mmake_firestore_writer.<locals>.write_batch_to_firestore\u001b[0;34m(batch_df, epoch_id)\u001b[0m\n\u001b[1;32m     97\u001b[0m ops_in_current_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ops_in_current_batch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m--> 100\u001b[0m     \u001b[43m_commit_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     docs_written \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ops_in_current_batch\n\u001b[1;32m    102\u001b[0m     ops_in_current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[36], line 49\u001b[0m, in \u001b[0;36m_commit_with_retries\u001b[0;34m(batch_obj, max_retries, base_backoff)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m         \u001b[43mbatch_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GoogleAPICallError, RetryError, \u001b[38;5;167;01mIOError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/firestore_v1/batch.py:61\u001b[0m, in \u001b[0;36mWriteBatch.commit\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Commit the changes accumulated in this batch.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    write result contains an ``update_time`` field.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m request, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_commit(retry, timeout)\n\u001b[0;32m---> 61\u001b[0m commit_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_firestore_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rpc_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_pbs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_results \u001b[38;5;241m=\u001b[39m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(commit_response\u001b[38;5;241m.\u001b[39mwrite_results)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/firestore_v1/services/firestore/client.py:1431\u001b[0m, in \u001b[0;36mFirestoreClient.commit\u001b[0;34m(self, request, database, writes, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1431\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/retry/retry_base.py:229\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m next_sleep \u001b[38;5;241m>\u001b[39m deadline:\n\u001b[1;32m    224\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    225\u001b[0m         error_list,\n\u001b[1;32m    226\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mTIMEOUT,\n\u001b[1;32m    227\u001b[0m         original_timeout,\n\u001b[1;32m    228\u001b[0m     )\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    230\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], next_sleep)\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m next_sleep\n",
      "\u001b[0;31mRetryError\u001b[0m: Timeout of 60.0s exceeded, last exception: 429 Quota exceeded."
     ]
    }
   ],
   "source": [
    "writer = make_firestore_writer(\"daily_plan\", db)\n",
    "\n",
    "# Run the batch write\n",
    "writer(all_plans,epoch_id=\"2025-11-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de358de1-ee49-4e0e-9d27-0194b1fb7b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
